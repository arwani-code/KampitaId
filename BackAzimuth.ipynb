{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74150420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhafran\\.conda\\envs\\ExVodka\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install --user tables\n",
    "# !pip install imblearn\n",
    "# !pip3 install --force-reinstall tensorflow==2.8.0 --user \n",
    "# !pip3 install --upgrade tensorflow-gpu --user\n",
    "# !pip3 install tensorflow_datasets --user\n",
    "# !pip3 install numba\n",
    "# !pip3 install PyWavelets\n",
    "# !pip3 install imageio\n",
    "# !pip3 install scikit-image\n",
    "# !pip3 install psutil\n",
    "\n",
    "import os, h5py, re, time, pywt, imageio, pickle, psutil, sys, json, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "from joblib import Parallel, delayed\n",
    "import numba as nb\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np   \n",
    "\n",
    "from skimage.restoration import (denoise_wavelet, estimate_sigma)\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output\n",
    "import typing\n",
    "\n",
    "from tensorflow_addons.utils.types import AcceptableDTypes\n",
    "from typeguard import typechecked\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd5cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhafran\\.conda\\envs\\ExVodka\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (7,11,13,14,15,18,19,20,21,22,24,25,26,30,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network_code</th>\n",
       "      <th>receiver_code</th>\n",
       "      <th>receiver_type</th>\n",
       "      <th>receiver_latitude</th>\n",
       "      <th>receiver_longitude</th>\n",
       "      <th>receiver_elevation_m</th>\n",
       "      <th>p_arrival_sample</th>\n",
       "      <th>p_status</th>\n",
       "      <th>p_weight</th>\n",
       "      <th>p_travel_sec</th>\n",
       "      <th>...</th>\n",
       "      <th>source_magnitude_author</th>\n",
       "      <th>source_mechanism_strike_dip_rake</th>\n",
       "      <th>source_distance_deg</th>\n",
       "      <th>source_distance_km</th>\n",
       "      <th>back_azimuth_deg</th>\n",
       "      <th>snr_db</th>\n",
       "      <th>coda_end_sample</th>\n",
       "      <th>trace_start_time</th>\n",
       "      <th>trace_category</th>\n",
       "      <th>trace_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-21 05:55:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201510210555_NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-06 14:50:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201511061450_NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-07 02:20:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201511070220_NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-14 05:15:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201511140515_NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TA</td>\n",
       "      <td>109C</td>\n",
       "      <td>HH</td>\n",
       "      <td>32.8889</td>\n",
       "      <td>-117.1051</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-25 18:50:00</td>\n",
       "      <td>noise</td>\n",
       "      <td>109C.TA_201512251850_NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  network_code receiver_code receiver_type  receiver_latitude  \\\n",
       "0           TA          109C            HH            32.8889   \n",
       "1           TA          109C            HH            32.8889   \n",
       "2           TA          109C            HH            32.8889   \n",
       "3           TA          109C            HH            32.8889   \n",
       "4           TA          109C            HH            32.8889   \n",
       "\n",
       "   receiver_longitude  receiver_elevation_m  p_arrival_sample p_status  \\\n",
       "0           -117.1051                 150.0               NaN      NaN   \n",
       "1           -117.1051                 150.0               NaN      NaN   \n",
       "2           -117.1051                 150.0               NaN      NaN   \n",
       "3           -117.1051                 150.0               NaN      NaN   \n",
       "4           -117.1051                 150.0               NaN      NaN   \n",
       "\n",
       "   p_weight  p_travel_sec  ...  source_magnitude_author  \\\n",
       "0       NaN           NaN  ...                      NaN   \n",
       "1       NaN           NaN  ...                      NaN   \n",
       "2       NaN           NaN  ...                      NaN   \n",
       "3       NaN           NaN  ...                      NaN   \n",
       "4       NaN           NaN  ...                      NaN   \n",
       "\n",
       "  source_mechanism_strike_dip_rake  source_distance_deg source_distance_km  \\\n",
       "0                              NaN                  NaN                NaN   \n",
       "1                              NaN                  NaN                NaN   \n",
       "2                              NaN                  NaN                NaN   \n",
       "3                              NaN                  NaN                NaN   \n",
       "4                              NaN                  NaN                NaN   \n",
       "\n",
       "  back_azimuth_deg snr_db  coda_end_sample     trace_start_time  \\\n",
       "0              NaN    NaN              NaN  2015-10-21 05:55:00   \n",
       "1              NaN    NaN              NaN  2015-11-06 14:50:00   \n",
       "2              NaN    NaN              NaN  2015-11-07 02:20:00   \n",
       "3              NaN    NaN              NaN  2015-11-14 05:15:00   \n",
       "4              NaN    NaN              NaN  2015-12-25 18:50:00   \n",
       "\n",
       "  trace_category               trace_name  \n",
       "0          noise  109C.TA_201510210555_NO  \n",
       "1          noise  109C.TA_201511061450_NO  \n",
       "2          noise  109C.TA_201511070220_NO  \n",
       "3          noise  109C.TA_201511140515_NO  \n",
       "4          noise  109C.TA_201512251850_NO  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('merge.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d46e279e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_origin_time</th>\n",
       "      <th>source_origin_uncertainty_sec</th>\n",
       "      <th>source_latitude</th>\n",
       "      <th>source_longitude</th>\n",
       "      <th>source_error_sec</th>\n",
       "      <th>source_gap_deg</th>\n",
       "      <th>source_horizontal_uncertainty_km</th>\n",
       "      <th>source_depth_km</th>\n",
       "      <th>source_depth_uncertainty_km</th>\n",
       "      <th>source_magnitude</th>\n",
       "      <th>source_magnitude_type</th>\n",
       "      <th>source_magnitude_author</th>\n",
       "      <th>source_mechanism_strike_dip_rake</th>\n",
       "      <th>source_distance_deg</th>\n",
       "      <th>source_distance_km</th>\n",
       "      <th>back_azimuth_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235426</th>\n",
       "      <td>8556349</td>\n",
       "      <td>2006-07-23 15:58:50.88</td>\n",
       "      <td>0.47</td>\n",
       "      <td>33.7496</td>\n",
       "      <td>-117.4938</td>\n",
       "      <td>1.1119</td>\n",
       "      <td>107.466</td>\n",
       "      <td>4.6403</td>\n",
       "      <td>0.45</td>\n",
       "      <td>None</td>\n",
       "      <td>3.6</td>\n",
       "      <td>ml</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.92</td>\n",
       "      <td>102.09</td>\n",
       "      <td>159.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235427</th>\n",
       "      <td>8860238</td>\n",
       "      <td>2006-11-03 15:56:42.73</td>\n",
       "      <td>0.24</td>\n",
       "      <td>32.7077</td>\n",
       "      <td>-116.0446</td>\n",
       "      <td>0.899</td>\n",
       "      <td>37.593</td>\n",
       "      <td>2.9542</td>\n",
       "      <td>9.2</td>\n",
       "      <td>None</td>\n",
       "      <td>4.3</td>\n",
       "      <td>mb</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.91</td>\n",
       "      <td>101.34</td>\n",
       "      <td>281.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235428</th>\n",
       "      <td>8940123</td>\n",
       "      <td>2006-11-03 16:12:12.44</td>\n",
       "      <td>0.27</td>\n",
       "      <td>32.7253</td>\n",
       "      <td>-116.0348</td>\n",
       "      <td>0.8127</td>\n",
       "      <td>48.096</td>\n",
       "      <td>3.0397</td>\n",
       "      <td>12.66</td>\n",
       "      <td>None</td>\n",
       "      <td>3.6</td>\n",
       "      <td>ml</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.92</td>\n",
       "      <td>101.87</td>\n",
       "      <td>280.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235429</th>\n",
       "      <td>9443190</td>\n",
       "      <td>2006-11-14 13:32:14.26</td>\n",
       "      <td>0.25</td>\n",
       "      <td>32.7063</td>\n",
       "      <td>-116.0241</td>\n",
       "      <td>0.9173</td>\n",
       "      <td>43.783</td>\n",
       "      <td>2.6112</td>\n",
       "      <td>11.5</td>\n",
       "      <td>None</td>\n",
       "      <td>3.8</td>\n",
       "      <td>ml</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.93</td>\n",
       "      <td>103.26</td>\n",
       "      <td>281.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235430</th>\n",
       "      <td>9443449</td>\n",
       "      <td>2006-11-27 10:46:29.92</td>\n",
       "      <td>0.67</td>\n",
       "      <td>31.9679</td>\n",
       "      <td>-117.1944</td>\n",
       "      <td>1.0362</td>\n",
       "      <td>182.145</td>\n",
       "      <td>5.6607</td>\n",
       "      <td>7.26</td>\n",
       "      <td>None</td>\n",
       "      <td>3.6</td>\n",
       "      <td>ml</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.92</td>\n",
       "      <td>102.48</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source_id      source_origin_time source_origin_uncertainty_sec  \\\n",
       "235426   8556349  2006-07-23 15:58:50.88                          0.47   \n",
       "235427   8860238  2006-11-03 15:56:42.73                          0.24   \n",
       "235428   8940123  2006-11-03 16:12:12.44                          0.27   \n",
       "235429   9443190  2006-11-14 13:32:14.26                          0.25   \n",
       "235430   9443449  2006-11-27 10:46:29.92                          0.67   \n",
       "\n",
       "        source_latitude  source_longitude source_error_sec source_gap_deg  \\\n",
       "235426          33.7496         -117.4938           1.1119        107.466   \n",
       "235427          32.7077         -116.0446            0.899         37.593   \n",
       "235428          32.7253         -116.0348           0.8127         48.096   \n",
       "235429          32.7063         -116.0241           0.9173         43.783   \n",
       "235430          31.9679         -117.1944           1.0362        182.145   \n",
       "\n",
       "       source_horizontal_uncertainty_km source_depth_km  \\\n",
       "235426                           4.6403            0.45   \n",
       "235427                           2.9542             9.2   \n",
       "235428                           3.0397           12.66   \n",
       "235429                           2.6112            11.5   \n",
       "235430                           5.6607            7.26   \n",
       "\n",
       "       source_depth_uncertainty_km  source_magnitude source_magnitude_type  \\\n",
       "235426                        None               3.6                    ml   \n",
       "235427                        None               4.3                    mb   \n",
       "235428                        None               3.6                    ml   \n",
       "235429                        None               3.8                    ml   \n",
       "235430                        None               3.6                    ml   \n",
       "\n",
       "       source_magnitude_author source_mechanism_strike_dip_rake  \\\n",
       "235426                    None                             None   \n",
       "235427                    None                             None   \n",
       "235428                    None                             None   \n",
       "235429                    None                             None   \n",
       "235430                    None                             None   \n",
       "\n",
       "        source_distance_deg  source_distance_km  back_azimuth_deg  \n",
       "235426                 0.92              102.09             159.3  \n",
       "235427                 0.91              101.34             281.7  \n",
       "235428                 0.92              101.87             280.5  \n",
       "235429                 0.93              103.26             281.6  \n",
       "235430                 0.92              102.48               4.7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 13:30][df['trace_category']!='noise'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e41fe10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='back_azimuth_deg', ylabel='Density'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df['back_azimuth_deg'][df['trace_category']!='noise'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b4045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomUnderSampler(df_earthquake, random_state=24, size_underSample=20000):\n",
    "    np.random.seed(random_state)\n",
    "    index = np.random.randint(0, len(df_earthquake)+1, size=size_underSample)\n",
    "    return df_earthquake.loc[index].reset_index(drop=True)\n",
    "\n",
    "df_noise = df[df['trace_category']=='noise'][['p_arrival_sample', 's_arrival_sample',\n",
    "                                              'source_magnitude', 'trace_name', 'trace_category']]\n",
    "df_noise = df_noise.reset_index(drop=True)\n",
    "df_noise = df_noise.fillna(-1)\n",
    "\n",
    "df_earthquake = df[df['trace_category']=='earthquake_local'][['p_arrival_sample', 's_arrival_sample', 'back_azimuth_deg',\n",
    "                                                              'source_magnitude', 'trace_name', 'trace_category']]\n",
    "df_earthquake = df_earthquake.reset_index(drop=True)\n",
    "df_earthquake_sample = RandomUnderSampler(df_earthquake, size_underSample=df_noise.shape[0],\n",
    "                                   random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aacc81b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_arrival_sample</th>\n",
       "      <th>s_arrival_sample</th>\n",
       "      <th>back_azimuth_deg</th>\n",
       "      <th>source_magnitude</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>trace_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>527399</th>\n",
       "      <td>400.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>114.400</td>\n",
       "      <td>1.93</td>\n",
       "      <td>GDXB.NC_20130721134219_EV</td>\n",
       "      <td>earthquake_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770755</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>116.427</td>\n",
       "      <td>3.50</td>\n",
       "      <td>OHAK.AT_20180927151929_EV</td>\n",
       "      <td>earthquake_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23292</th>\n",
       "      <td>500.0</td>\n",
       "      <td>1893.0</td>\n",
       "      <td>22.200</td>\n",
       "      <td>3.30</td>\n",
       "      <td>ALN.HT_20100914071737_EV</td>\n",
       "      <td>earthquake_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60135</th>\n",
       "      <td>700.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>31.600</td>\n",
       "      <td>1.46</td>\n",
       "      <td>B067.PB_20141204201722_EV</td>\n",
       "      <td>earthquake_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607313</th>\n",
       "      <td>700.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>341.500</td>\n",
       "      <td>1.67</td>\n",
       "      <td>ID06.GS_20170912111559_EV</td>\n",
       "      <td>earthquake_local</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        p_arrival_sample  s_arrival_sample  back_azimuth_deg  \\\n",
       "527399             400.0             473.0           114.400   \n",
       "770755            1000.0            3768.0           116.427   \n",
       "23292              500.0            1893.0            22.200   \n",
       "60135              700.0             809.0            31.600   \n",
       "607313             700.0             792.0           341.500   \n",
       "\n",
       "        source_magnitude                 trace_name    trace_category  \n",
       "527399              1.93  GDXB.NC_20130721134219_EV  earthquake_local  \n",
       "770755              3.50  OHAK.AT_20180927151929_EV  earthquake_local  \n",
       "23292               3.30   ALN.HT_20100914071737_EV  earthquake_local  \n",
       "60135               1.46  B067.PB_20141204201722_EV  earthquake_local  \n",
       "607313              1.67  ID06.GS_20170912111559_EV  earthquake_local  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "None\n",
      "\n",
      "DataFrame 2 Shape       : (1030231, 6)\n",
      "DataFrame 2 Sliced Shape: (412092, 6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = pd.concat([df_earthquake_sample, df_noise]).reset_index(drop=True)\n",
    "rate = .4 # 0<=rate<=1, ketika rate 1 mengambil seluruh data (balanced), ketika 0 tidak ada data yg di ambil\n",
    "df_2_slice = df_earthquake.groupby('trace_category', group_keys=False).apply(lambda x: x.sample(int(len(df_earthquake)*rate))) \n",
    "\n",
    "print(f'''\n",
    "{display(df_earthquake.sample(5))}\n",
    "\n",
    "DataFrame 2 Shape       : {df_earthquake.shape}\n",
    "DataFrame 2 Sliced Shape: {df_2_slice.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b281157d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = np.zeros(500)\n",
    "X2[int(round(250*(data.iloc[0]/np.pi + 1)))%500] = 1\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d3c21149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7771679057733776"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.max(X2)\n",
    "for i in range(len(X2)):\n",
    "    if abs(X2[i]-M) < 1e-5:\n",
    "        angle = np.pi*i/250 - np.pi\n",
    "        break\n",
    "# angle = np.pi*np.dot(np.array([i for i in range(500)]),X2)/500  # Averaging\n",
    "angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4cd04586",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16196/2796945105.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "X2 = np.zeros(500)\n",
    "X2[int(round(250*(data.iloc[0]/np.pi + 1)))%500] = 1\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de87dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db438680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhafran\\AppData\\Local\\Temp/ipykernel_16196/2226519839.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y['s_arrival_sample'] = y['s_arrival_sample'].map(lambda x: np.log(x))\n",
      "C:\\Users\\Zhafran\\AppData\\Local\\Temp/ipykernel_16196/2226519839.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y['source_magnitude'] = y['source_magnitude'].map(lambda x: np.log1p(x))\n",
      "C:\\Users\\Zhafran\\AppData\\Local\\Temp/ipykernel_16196/2226519839.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y['back_azimuth_deg'] = y['back_azimuth_deg'].map(lambda x: np.radians(x-180))\n",
      "C:\\Users\\Zhafran\\AppData\\Local\\Temp/ipykernel_16196/2226519839.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y['sin'] = y['back_azimuth_deg'].map(lambda x: np.sin(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train Shape: (247255,)\n",
      "X_test  Shape: (82418,)\n",
      "X_val   Shape: (82419,)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhafran\\AppData\\Local\\Temp/ipykernel_16196/2226519839.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y['cos'] = y['back_azimuth_deg'].map(lambda x: np.cos(x))\n"
     ]
    }
   ],
   "source": [
    "X = df_2_slice['trace_name']\n",
    "y = df_2_slice[['s_arrival_sample', 'source_magnitude', 'back_azimuth_deg']]\n",
    "y['s_arrival_sample'] = y['s_arrival_sample'].map(lambda x: np.log(x))\n",
    "y['source_magnitude'] = y['source_magnitude'].map(lambda x: np.log1p(x))\n",
    "y['back_azimuth_deg'] = y['back_azimuth_deg'].map(lambda x: np.radians(x-180))\n",
    "y['sin'] = y['back_azimuth_deg'].map(lambda x: np.sin(x))\n",
    "y['cos'] = y['back_azimuth_deg'].map(lambda x: np.cos(x))\n",
    "# y_back_az = y['back_azimuth_deg']\n",
    "# y.drop(columns='back_azimuth_deg', inplace=True)\n",
    "\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, train_size=0.6)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, train_size=0.5)\n",
    "\n",
    "y_train_backAzimuth = y_train['back_azimuth_deg']\n",
    "y_val_backAzimuth = y_val['back_azimuth_deg']\n",
    "y_test_backAzimuth = y_test['back_azimuth_deg']\n",
    "\n",
    "y_train.drop(columns='back_azimuth_deg', inplace=True)\n",
    "y_val.drop(columns='back_azimuth_deg', inplace=True)\n",
    "y_test.drop(columns='back_azimuth_deg', inplace=True)\n",
    "\n",
    "print(f'''\n",
    "X_train Shape: {X_train.shape}\n",
    "X_test  Shape: {X_test.shape}\n",
    "X_val   Shape: {X_val.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d79a8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveform(trace_name):\n",
    "    '''\n",
    "    Kolom 1: East-West Channel\n",
    "    Kolom 2: North-South Channel\n",
    "    Kolom 3: Z (Vertical) Channel\n",
    "    '''\n",
    "    filename = \"merge.hdf5\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        data = f.get('data/'+trace_name)\n",
    "        data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631367ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "def DenoiseWavelet(data, type='BayesShrink'):\n",
    "    def BayesShrink():\n",
    "        im_bayes = denoise_wavelet(np.array(data), convert2ycbcr=True, multichannel=True,\n",
    "                                   method='BayesShrink', mode='soft', \n",
    "                                   rescale_sigma=True, wavelet_levels=4)\n",
    "        return im_bayes\n",
    "    \n",
    "    def VisuShrink():\n",
    "        sigma_est = estimate_sigma(np.array(data), multichannel=True, average_sigmas=True)\n",
    "        im_visu = denoise_wavelet(np.array(img), convert2ycbcr=True, multichannel=True,\n",
    "                                  method='VisuShrink', mode='soft', wavelet_levels=4,\n",
    "                                  sigma=sigma_est, rescale_sigma=True)\n",
    "        \n",
    "        return im_visu\n",
    "    \n",
    "    if type=='BayesShrink':\n",
    "        return BayesShrink()\n",
    "    elif type=='VisuShrink':\n",
    "        return VisuShrink()\n",
    "\n",
    "def create_data_2(data_feature, data_label, BATCH=128):\n",
    "    def create_feature():\n",
    "        rs = []\n",
    "        for t in tqdm(data_feature.values):\n",
    "            data_l = get_waveform(t)\n",
    "            data_denoise = DenoiseWavelet(data_l, type='BayesShrink')\n",
    "            rs.append(data_denoise)\n",
    "        return np.array(rs)\n",
    "    \n",
    "    def create_label():\n",
    "        return np.array(data_label.values)\n",
    "    \n",
    "    def create_dataset(feature, label): \n",
    "        gen = DataGenerator(feature, label, BATCH)\n",
    "        return gen\n",
    "    \n",
    "    feature = create_feature()\n",
    "    label = create_label()\n",
    "    \n",
    "    ts_dt = create_dataset(feature, label)\n",
    "    print(f'\\n{ts_dt}\\n')\n",
    "    return ts_dt # {'feature':feature, 'label':label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3321c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/247255 [00:00<?, ?it/s]C:\\Users\\Zhafran\\AppData\\Local\\Temp/ipykernel_16196/3254799232.py:16: FutureWarning: `multichannel` is a deprecated argument name for `denoise_wavelet`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  im_bayes = denoise_wavelet(np.array(data), convert2ycbcr=True, multichannel=True,\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247255/247255 [2:11:44<00:00, 31.28it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<__main__.DataGenerator object at 0x000002073674E6D0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_train = create_data_2(data_feature=X_train,\n",
    "                          data_label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1549bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/82419 [00:00<?, ?it/s]C:\\Users\\Zhafran\\AppData\\Local\\Temp/ipykernel_16196/3254799232.py:16: FutureWarning: `multichannel` is a deprecated argument name for `denoise_wavelet`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  im_bayes = denoise_wavelet(np.array(data), convert2ycbcr=True, multichannel=True,\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82419/82419 [44:44<00:00, 30.70it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<__main__.DataGenerator object at 0x0000020736747E20>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_val = create_data_2(data_feature=X_val,\n",
    "                          data_label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c11ba33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/82418 [00:00<?, ?it/s]C:\\Users\\Zhafran\\AppData\\Local\\Temp/ipykernel_16196/3254799232.py:16: FutureWarning: `multichannel` is a deprecated argument name for `denoise_wavelet`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  im_bayes = denoise_wavelet(np.array(data), convert2ycbcr=True, multichannel=True,\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82418/82418 [47:50<00:00, 28.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<__main__.DataGenerator object at 0x0000020775D11C10>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_test = create_data_2(data_feature=X_test,\n",
    "                          data_label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e89e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "datas_ = [dt_train, dt_val, dt_test]\n",
    "types_ = ['Train', 'Val', 'Test']\n",
    "\n",
    "for data_, type_ in zip(datas_, types_):\n",
    "    data_json = {}\n",
    "    data_json.update({\n",
    "        'X':data_.x,\n",
    "        'y':data_.y,\n",
    "        'Batch':data_.batch_size\n",
    "    })\n",
    "    path = os.path.join(os.getcwd(), 'Prefetch Dataset', 'v3 - 40% - BackAzimuth', f'{type_}.json')\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(data_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfdd4301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "path = os.path.join(os.getcwd(), 'Prefetch Dataset', 'v3 - 40% - BackAzimuth')\n",
    "\n",
    "with open(os.path.join(path, 'Train.json'), 'rb') as fp:\n",
    "    dt_train = pickle.load(fp)\n",
    "\n",
    "with open(os.path.join(path, 'Val.json'), 'rb') as fp:\n",
    "    dt_val = pickle.load(fp)\n",
    "\n",
    "with open(os.path.join(path, 'Test.json'), 'rb') as fp:\n",
    "    dt_test = pickle.load(fp)\n",
    "\n",
    "dt_train = DataGenerator(dt_train['X'], dt_train['y'], 24)\n",
    "dt_val = DataGenerator(dt_val['X'], dt_val['y'], 24)\n",
    "dt_test = DataGenerator(dt_test['X'], dt_test['y'], 24)\n",
    "\n",
    "dt_train = DataGenerator(dt_train.x, dt_train.y[:, 2:], 24)\n",
    "dt_val = DataGenerator(dt_val.x, dt_val.y[:, 2:], 24)\n",
    "dt_test = DataGenerator(dt_test.x, dt_test.y[:, 2:], 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c82f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98685572, -0.16160382],\n",
       "       [ 0.85081111, -0.52547165],\n",
       "       [ 0.04693211,  0.99889808],\n",
       "       ...,\n",
       "       [-0.89879405,  0.43837115],\n",
       "       [ 0.99924268,  0.03891102],\n",
       "       [ 0.11389683, -0.99349258]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_train.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24bfabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_atan2(data):\n",
    "    sin_ = data[:, 0]\n",
    "    cos_ = data[:, 1]\n",
    "    radian_backazimuth = []\n",
    "    for i in range(data.shape[0]):\n",
    "        radian_backazimuth.append(math.atan2(sin_[i], cos_[i]))\n",
    "    return np.array(radian_backazimuth)\n",
    "\n",
    "y_train_backAzimuth = np.degrees(y_train_backAzimuth)\n",
    "y_val_backAzimuth = np.degrees(y_val_backAzimuth)\n",
    "\n",
    "y_train_backAzimuth = y_train_backAzimuth+180\n",
    "y_val_backAzimuth = y_val_backAzimuth+180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f6a04fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_bins(data, bins, benchmark=False):\n",
    "    bins += 1\n",
    "    bins_degree = 360/bins\n",
    "    box_azimuth = []\n",
    "    for idx_ in range(len(data)):\n",
    "        try:\n",
    "            box_azimuth_zero = np.zeros(bins)\n",
    "            box_azimuth_zero[int(data.iloc[idx_]//bins_degree)] = 1\n",
    "            box_azimuth.append(box_azimuth_zero)\n",
    "        except:\n",
    "            print(int(data.iloc[idx_]//bins_degree))\n",
    "    box_azimuth = np.array(box_azimuth)\n",
    "    \n",
    "    if benchmark:\n",
    "        error = []\n",
    "        for idx_ in range(len(data)):\n",
    "            itemindex = np.where(box_azimuth[idx_]==1)\n",
    "            itemindex = itemindex[0]*bins_degree+bins_degree/2\n",
    "            error_loop = (((data.iloc[idx_]-itemindex[0])/data.iloc[idx_])**2)**0.5\n",
    "            print(f'Real: {data.iloc[idx_]}\\nBox_: {itemindex}')\n",
    "            if idx_==5:\n",
    "                break\n",
    "#             error.append(error_loop)\n",
    "        print('')\n",
    "    return box_azimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "575572f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "Train : (247251, 3)\n",
      "Val   : (82418, 3)\n",
      "Wall time: 3.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "box_y_train = box_bins(y_train_backAzimuth, 2, benchmark=False)\n",
    "box_y_val = box_bins(y_val_backAzimuth, 2, benchmark=False)\n",
    "box_y_test = box_bins(y_test_backAzimuth, 2, benchmark=False)\n",
    "\n",
    "print(f'Train : {box_y_train.shape}\\nVal   : {box_y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e008f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a1aad755",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_box = DataGenerator(dt_train.x, box_y_train, 196)\n",
    "dt_val_box = DataGenerator(dt_val.x, box_y_val, 196)\n",
    "dt_test_box = DataGenerator(dt_test.x, box_y_val, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74945b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1a575e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project C:\\Users\\Zhafran\\Documents\\Data Science\\BANGKIT - Capstone - Earthquake\\HyperParameter Tuner Log\\HyperParam Tuner S-Wave and Magnitude -2-\\oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project C:\\Users\\Zhafran\\Documents\\Data Science\\BANGKIT - Capstone - Earthquake\\HyperParameter Tuner Log\\HyperParam Tuner S-Wave and Magnitude -2-\\oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from C:\\Users\\Zhafran\\Documents\\Data Science\\BANGKIT - Capstone - Earthquake\\HyperParameter Tuner Log\\HyperParam Tuner S-Wave and Magnitude -2-\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from C:\\Users\\Zhafran\\Documents\\Data Science\\BANGKIT - Capstone - Earthquake\\HyperParameter Tuner Log\\HyperParam Tuner S-Wave and Magnitude -2-\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Input((6000, 3)))\n",
    "    \n",
    "    for layer_conv in range(hp.Int('layer_conv_', 1, 5)):\n",
    "        hp_unit = hp.Int('units_'+str(layer_conv), 32, 256, step=32)\n",
    "        hp_kernel = hp.Int('kernels_'+str(layer_conv), 1, 9, step=1)\n",
    "        is_activation = hp.Choice('is_activation', ['ReLU', 'LeakyReLU'])\n",
    "        alpha_leaky_relu = hp.Float('alpha_leakyReLU_'+str(layer_conv),\n",
    "                                    0.1, 0.5, step=0.1)\n",
    "        is_pool = hp.Choice(\"is_pool\", [\"avg\", \"max\"])\n",
    "\n",
    "        \n",
    "        model.add(tf.keras.layers.Conv1D(filters=hp_unit,\n",
    "                                         kernel_size=hp_kernel))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        if is_activation=='ReLU':\n",
    "            with hp.conditional_scope('is_activation', ['ReLU']):\n",
    "                model.add(tf.keras.layers.Activation('relu'))\n",
    "        elif is_activation=='LeakyReLU':\n",
    "            with hp.conditional_scope('is_activation', ['LeakyReLU']):\n",
    "                model.add(tf.keras.layers.LeakyReLU(alpha=alpha_leaky_relu))\n",
    "                \n",
    "        if is_pool == \"avg\":\n",
    "            with hp.conditional_scope(\"is_pool\", [\"avg\"]):\n",
    "                model.add(tf.keras.layers.AveragePooling1D())\n",
    "        if is_pool == \"max\":\n",
    "            with hp.conditional_scope(\"is_pool\", [\"max\"]):\n",
    "                model.add(tf.keras.layers.MaxPooling1D())\n",
    "    \n",
    "    hp_unit_gru = hp.Int('units_gru_', 64, 128, step=32)\n",
    "    model.add(tf.keras.layers.GRU(units=hp_unit_gru, return_sequences=True)) \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    for layer_dense in range(hp.Int('layer_dense_', 2, 4)):\n",
    "        hp_unit_dense = hp.Int('units_'+str(layer_dense), 64, 256, step=32)\n",
    "        hp_unit_dropout_dense = hp.Float('dropout_dense_', 0.1, 0.5, step=0.1)\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(units=hp_unit_dense))\n",
    "        model.add(tf.keras.layers.Dropout(rate=hp_unit_dropout_dense))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(2))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), #tfa.optimizers.LazyAdam(),\n",
    "              loss=tf.keras.losses.MeanSquaredError())\n",
    "    return model\n",
    "\n",
    "tuner = kt.BayesianOptimization(model_builder,\n",
    "                     objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "                     max_trials=20,\n",
    "                     project_name='HyperParam Tuner S-Wave and Magnitude -2-',\n",
    "                     directory=os.path.join(os.getcwd(), 'HyperParameter Tuner Log'))\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                                 patience=5, min_lr=0.0001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# tuner.search(dt_train, validation_data=dt_val,\n",
    "#              epochs=15,\n",
    "#              use_multiprocessing=True,\n",
    "#              workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0a13f4d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 6000, 32)          128       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 6000, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6000, 32)          0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 3000, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 3000, 256)         8448      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 3000, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3000, 256)         0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 1500, 256)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1500, 32)          8224      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1500, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1500, 32)          0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 750, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 750, 64)           2112      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 750, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 750, 64)           0         \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 375, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 369, 256)          114944    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 369, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 369, 256)          0         \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 184, 256)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 184, 64)           61824     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 11776)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                376864    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               8448      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 584,066\n",
      "Trainable params: 582,786\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "# model.save('model-visu.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d587f3fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 6000, 3)]         0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 6000, 32)          128       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 6000, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6000, 32)          0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 3000, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 3000, 256)         8448      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 3000, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3000, 256)         0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 1500, 256)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1500, 32)          8224      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1500, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1500, 32)          0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 750, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 750, 64)           2112      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 750, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 750, 64)           0         \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 375, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 369, 256)          114944    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 369, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 369, 256)          0         \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 184, 256)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 184, 64)           61824     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 11776)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                376864    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               8448      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 361)               92777     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 676,329\n",
      "Trainable params: 675,049\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = model.layers[-2].output\n",
    "x = tf.keras.layers.Dense(361, activation='softmax')(x)\n",
    "\n",
    "new_model = tf.keras.Model(inputs=model.input, outputs=x)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "760bf53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train = DataGenerator(dt_train.x, dt_train.y, 196)\n",
    "dt_val = DataGenerator(dt_val.x, dt_val.y, 196)\n",
    "dt_test = DataGenerator(dt_test.x, dt_test.y, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8fe00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "488bb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_atan2(x, y, epsilon=1.0e-12):\n",
    "    x = tf.where(tf.equal(x, 0.0), x+epsilon, x)\n",
    "    y = tf.where(tf.equal(y, 0.0), y+epsilon, y)    \n",
    "    angle = tf.where(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))\n",
    "    angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\n",
    "    angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\n",
    "    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\n",
    "    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\n",
    "    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), tf.zeros_like(x), angle)\n",
    "    return angle\n",
    "\n",
    "# y in radians\n",
    "def rmse_360_2(y_true, y_pred):\n",
    "    return K.mean(K.abs(tf_atan2(K.sin(y_true - y_pred), K.cos(y_true - y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b89cd88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 6000, 3)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_156 (Conv1D)            (None, 3000, 64)     1408        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_158 (Batch  (None, 3000, 64)    256         ['conv1d_156[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_158 (Activation)    (None, 3000, 64)     0           ['batch_normalization_158[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 1500, 64)    0           ['activation_158[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_159 (Batch  (None, 1500, 64)    256         ['max_pooling1d_2[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_159 (Activation)    (None, 1500, 64)     0           ['batch_normalization_159[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_157 (Conv1D)            (None, 1500, 128)    8320        ['activation_159[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 1500, 128)   512         ['conv1d_157[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_160 (Activation)    (None, 1500, 128)    0           ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_158 (Conv1D)            (None, 1500, 32)     12320       ['activation_160[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_74 (Concatenate)   (None, 1500, 96)     0           ['max_pooling1d_2[0][0]',        \n",
      "                                                                  'conv1d_158[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 1500, 96)    384         ['concatenate_74[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_161 (Activation)    (None, 1500, 96)     0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_159 (Conv1D)            (None, 1500, 128)    12416       ['activation_161[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 1500, 128)   512         ['conv1d_159[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_162 (Activation)    (None, 1500, 128)    0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_160 (Conv1D)            (None, 1500, 32)     12320       ['activation_162[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_75 (Concatenate)   (None, 1500, 128)    0           ['max_pooling1d_2[0][0]',        \n",
      "                                                                  'conv1d_158[0][0]',             \n",
      "                                                                  'conv1d_160[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_163 (Batch  (None, 1500, 128)   512         ['concatenate_75[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_163 (Activation)    (None, 1500, 128)    0           ['batch_normalization_163[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_161 (Conv1D)            (None, 1500, 128)    16512       ['activation_163[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_164 (Batch  (None, 1500, 128)   512         ['conv1d_161[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_164 (Activation)    (None, 1500, 128)    0           ['batch_normalization_164[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_162 (Conv1D)            (None, 1500, 32)     12320       ['activation_164[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_76 (Concatenate)   (None, 1500, 160)    0           ['max_pooling1d_2[0][0]',        \n",
      "                                                                  'conv1d_158[0][0]',             \n",
      "                                                                  'conv1d_160[0][0]',             \n",
      "                                                                  'conv1d_162[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 1500, 160)   640         ['concatenate_76[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_165 (Activation)    (None, 1500, 160)    0           ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_163 (Conv1D)            (None, 1500, 128)    20608       ['activation_165[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 1500, 128)   512         ['conv1d_163[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_166 (Activation)    (None, 1500, 128)    0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_164 (Conv1D)            (None, 1500, 32)     12320       ['activation_166[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_77 (Concatenate)   (None, 1500, 192)    0           ['max_pooling1d_2[0][0]',        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv1d_158[0][0]',             \n",
      "                                                                  'conv1d_160[0][0]',             \n",
      "                                                                  'conv1d_162[0][0]',             \n",
      "                                                                  'conv1d_164[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 1500, 192)   768         ['concatenate_77[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_167 (Activation)    (None, 1500, 192)    0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_165 (Conv1D)            (None, 1500, 128)    24704       ['activation_167[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_168 (Batch  (None, 1500, 128)   512         ['conv1d_165[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_168 (Activation)    (None, 1500, 128)    0           ['batch_normalization_168[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_166 (Conv1D)            (None, 1500, 32)     12320       ['activation_168[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_78 (Concatenate)   (None, 1500, 224)    0           ['max_pooling1d_2[0][0]',        \n",
      "                                                                  'conv1d_158[0][0]',             \n",
      "                                                                  'conv1d_160[0][0]',             \n",
      "                                                                  'conv1d_162[0][0]',             \n",
      "                                                                  'conv1d_164[0][0]',             \n",
      "                                                                  'conv1d_166[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_169 (Batch  (None, 1500, 224)   896         ['concatenate_78[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_169 (Activation)    (None, 1500, 224)    0           ['batch_normalization_169[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_167 (Conv1D)            (None, 1500, 128)    28800       ['activation_169[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 1500, 128)   512         ['conv1d_167[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_170 (Activation)    (None, 1500, 128)    0           ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_168 (Conv1D)            (None, 1500, 32)     12320       ['activation_170[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_79 (Concatenate)   (None, 1500, 256)    0           ['max_pooling1d_2[0][0]',        \n",
      "                                                                  'conv1d_158[0][0]',             \n",
      "                                                                  'conv1d_160[0][0]',             \n",
      "                                                                  'conv1d_162[0][0]',             \n",
      "                                                                  'conv1d_164[0][0]',             \n",
      "                                                                  'conv1d_166[0][0]',             \n",
      "                                                                  'conv1d_168[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 1500, 256)   1024        ['concatenate_79[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_171 (Activation)    (None, 1500, 256)    0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_169 (Conv1D)            (None, 1500, 128)    32896       ['activation_171[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling1d_6 (AveragePo  (None, 750, 128)    0           ['conv1d_169[0][0]']             \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 750, 128)    512         ['average_pooling1d_6[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_172 (Activation)    (None, 750, 128)     0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_170 (Conv1D)            (None, 750, 128)     16512       ['activation_172[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_173 (Batch  (None, 750, 128)    512         ['conv1d_170[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_173 (Activation)    (None, 750, 128)     0           ['batch_normalization_173[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_171 (Conv1D)            (None, 750, 32)      12320       ['activation_173[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_80 (Concatenate)   (None, 750, 160)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_174 (Batch  (None, 750, 160)    640         ['concatenate_80[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_174 (Activation)    (None, 750, 160)     0           ['batch_normalization_174[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_172 (Conv1D)            (None, 750, 128)     20608       ['activation_174[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 750, 128)    512         ['conv1d_172[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_175 (Activation)    (None, 750, 128)     0           ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_173 (Conv1D)            (None, 750, 32)      12320       ['activation_175[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_81 (Concatenate)   (None, 750, 192)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 750, 192)    768         ['concatenate_81[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_176 (Activation)    (None, 750, 192)     0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_174 (Conv1D)            (None, 750, 128)     24704       ['activation_176[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 750, 128)    512         ['conv1d_174[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_177 (Activation)    (None, 750, 128)     0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_175 (Conv1D)            (None, 750, 32)      12320       ['activation_177[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_82 (Concatenate)   (None, 750, 224)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_178 (Batch  (None, 750, 224)    896         ['concatenate_82[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_178 (Activation)    (None, 750, 224)     0           ['batch_normalization_178[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_176 (Conv1D)            (None, 750, 128)     28800       ['activation_178[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_179 (Batch  (None, 750, 128)    512         ['conv1d_176[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_179 (Activation)    (None, 750, 128)     0           ['batch_normalization_179[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_177 (Conv1D)            (None, 750, 32)      12320       ['activation_179[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_83 (Concatenate)   (None, 750, 256)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]',             \n",
      "                                                                  'conv1d_177[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_180 (Batch  (None, 750, 256)    1024        ['concatenate_83[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_180 (Activation)    (None, 750, 256)     0           ['batch_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_178 (Conv1D)            (None, 750, 128)     32896       ['activation_180[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_181 (Batch  (None, 750, 128)    512         ['conv1d_178[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_181 (Activation)    (None, 750, 128)     0           ['batch_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_179 (Conv1D)            (None, 750, 32)      12320       ['activation_181[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_84 (Concatenate)   (None, 750, 288)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]',             \n",
      "                                                                  'conv1d_177[0][0]',             \n",
      "                                                                  'conv1d_179[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_182 (Batch  (None, 750, 288)    1152        ['concatenate_84[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_182 (Activation)    (None, 750, 288)     0           ['batch_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_180 (Conv1D)            (None, 750, 128)     36992       ['activation_182[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_183 (Batch  (None, 750, 128)    512         ['conv1d_180[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_183 (Activation)    (None, 750, 128)     0           ['batch_normalization_183[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_181 (Conv1D)            (None, 750, 32)      12320       ['activation_183[0][0]']         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " concatenate_85 (Concatenate)   (None, 750, 320)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]',             \n",
      "                                                                  'conv1d_177[0][0]',             \n",
      "                                                                  'conv1d_179[0][0]',             \n",
      "                                                                  'conv1d_181[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_184 (Batch  (None, 750, 320)    1280        ['concatenate_85[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_184 (Activation)    (None, 750, 320)     0           ['batch_normalization_184[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_182 (Conv1D)            (None, 750, 128)     41088       ['activation_184[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_185 (Batch  (None, 750, 128)    512         ['conv1d_182[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_185 (Activation)    (None, 750, 128)     0           ['batch_normalization_185[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_183 (Conv1D)            (None, 750, 32)      12320       ['activation_185[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_86 (Concatenate)   (None, 750, 352)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]',             \n",
      "                                                                  'conv1d_177[0][0]',             \n",
      "                                                                  'conv1d_179[0][0]',             \n",
      "                                                                  'conv1d_181[0][0]',             \n",
      "                                                                  'conv1d_183[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_186 (Batch  (None, 750, 352)    1408        ['concatenate_86[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_186 (Activation)    (None, 750, 352)     0           ['batch_normalization_186[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_184 (Conv1D)            (None, 750, 128)     45184       ['activation_186[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_187 (Batch  (None, 750, 128)    512         ['conv1d_184[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_187 (Activation)    (None, 750, 128)     0           ['batch_normalization_187[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_185 (Conv1D)            (None, 750, 32)      12320       ['activation_187[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_87 (Concatenate)   (None, 750, 384)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]',             \n",
      "                                                                  'conv1d_177[0][0]',             \n",
      "                                                                  'conv1d_179[0][0]',             \n",
      "                                                                  'conv1d_181[0][0]',             \n",
      "                                                                  'conv1d_183[0][0]',             \n",
      "                                                                  'conv1d_185[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_188 (Batch  (None, 750, 384)    1536        ['concatenate_87[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_188 (Activation)    (None, 750, 384)     0           ['batch_normalization_188[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_186 (Conv1D)            (None, 750, 128)     49280       ['activation_188[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_189 (Batch  (None, 750, 128)    512         ['conv1d_186[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_189 (Activation)    (None, 750, 128)     0           ['batch_normalization_189[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_187 (Conv1D)            (None, 750, 32)      12320       ['activation_189[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_88 (Concatenate)   (None, 750, 416)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]',             \n",
      "                                                                  'conv1d_177[0][0]',             \n",
      "                                                                  'conv1d_179[0][0]',             \n",
      "                                                                  'conv1d_181[0][0]',             \n",
      "                                                                  'conv1d_183[0][0]',             \n",
      "                                                                  'conv1d_185[0][0]',             \n",
      "                                                                  'conv1d_187[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_190 (Batch  (None, 750, 416)    1664        ['concatenate_88[0][0]']         \n",
      " Normalization)                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_190 (Activation)    (None, 750, 416)     0           ['batch_normalization_190[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_188 (Conv1D)            (None, 750, 128)     53376       ['activation_190[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_191 (Batch  (None, 750, 128)    512         ['conv1d_188[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_191 (Activation)    (None, 750, 128)     0           ['batch_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_189 (Conv1D)            (None, 750, 32)      12320       ['activation_191[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_89 (Concatenate)   (None, 750, 448)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]',             \n",
      "                                                                  'conv1d_177[0][0]',             \n",
      "                                                                  'conv1d_179[0][0]',             \n",
      "                                                                  'conv1d_181[0][0]',             \n",
      "                                                                  'conv1d_183[0][0]',             \n",
      "                                                                  'conv1d_185[0][0]',             \n",
      "                                                                  'conv1d_187[0][0]',             \n",
      "                                                                  'conv1d_189[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_192 (Batch  (None, 750, 448)    1792        ['concatenate_89[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_192 (Activation)    (None, 750, 448)     0           ['batch_normalization_192[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_190 (Conv1D)            (None, 750, 128)     57472       ['activation_192[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_193 (Batch  (None, 750, 128)    512         ['conv1d_190[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_193 (Activation)    (None, 750, 128)     0           ['batch_normalization_193[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_191 (Conv1D)            (None, 750, 32)      12320       ['activation_193[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_90 (Concatenate)   (None, 750, 480)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]',             \n",
      "                                                                  'conv1d_177[0][0]',             \n",
      "                                                                  'conv1d_179[0][0]',             \n",
      "                                                                  'conv1d_181[0][0]',             \n",
      "                                                                  'conv1d_183[0][0]',             \n",
      "                                                                  'conv1d_185[0][0]',             \n",
      "                                                                  'conv1d_187[0][0]',             \n",
      "                                                                  'conv1d_189[0][0]',             \n",
      "                                                                  'conv1d_191[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_194 (Batch  (None, 750, 480)    1920        ['concatenate_90[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_194 (Activation)    (None, 750, 480)     0           ['batch_normalization_194[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_192 (Conv1D)            (None, 750, 128)     61568       ['activation_194[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_195 (Batch  (None, 750, 128)    512         ['conv1d_192[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_195 (Activation)    (None, 750, 128)     0           ['batch_normalization_195[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_193 (Conv1D)            (None, 750, 32)      12320       ['activation_195[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_91 (Concatenate)   (None, 750, 512)     0           ['average_pooling1d_6[0][0]',    \n",
      "                                                                  'conv1d_171[0][0]',             \n",
      "                                                                  'conv1d_173[0][0]',             \n",
      "                                                                  'conv1d_175[0][0]',             \n",
      "                                                                  'conv1d_177[0][0]',             \n",
      "                                                                  'conv1d_179[0][0]',             \n",
      "                                                                  'conv1d_181[0][0]',             \n",
      "                                                                  'conv1d_183[0][0]',             \n",
      "                                                                  'conv1d_185[0][0]',             \n",
      "                                                                  'conv1d_187[0][0]',             \n",
      "                                                                  'conv1d_189[0][0]',             \n",
      "                                                                  'conv1d_191[0][0]',             \n",
      "                                                                  'conv1d_193[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_196 (Batch  (None, 750, 512)    2048        ['concatenate_91[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_196 (Activation)    (None, 750, 512)     0           ['batch_normalization_196[0][0]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv1d_194 (Conv1D)            (None, 750, 256)     131328      ['activation_196[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling1d_7 (AveragePo  (None, 375, 256)    0           ['conv1d_194[0][0]']             \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_197 (Batch  (None, 375, 256)    1024        ['average_pooling1d_7[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_197 (Activation)    (None, 375, 256)     0           ['batch_normalization_197[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_195 (Conv1D)            (None, 375, 128)     32896       ['activation_197[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_198 (Batch  (None, 375, 128)    512         ['conv1d_195[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_198 (Activation)    (None, 375, 128)     0           ['batch_normalization_198[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_196 (Conv1D)            (None, 375, 32)      12320       ['activation_198[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_92 (Concatenate)   (None, 375, 288)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_199 (Batch  (None, 375, 288)    1152        ['concatenate_92[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_199 (Activation)    (None, 375, 288)     0           ['batch_normalization_199[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_197 (Conv1D)            (None, 375, 128)     36992       ['activation_199[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_200 (Batch  (None, 375, 128)    512         ['conv1d_197[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_200 (Activation)    (None, 375, 128)     0           ['batch_normalization_200[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_198 (Conv1D)            (None, 375, 32)      12320       ['activation_200[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_93 (Concatenate)   (None, 375, 320)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_201 (Batch  (None, 375, 320)    1280        ['concatenate_93[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_201 (Activation)    (None, 375, 320)     0           ['batch_normalization_201[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_199 (Conv1D)            (None, 375, 128)     41088       ['activation_201[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_202 (Batch  (None, 375, 128)    512         ['conv1d_199[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_202 (Activation)    (None, 375, 128)     0           ['batch_normalization_202[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_200 (Conv1D)            (None, 375, 32)      12320       ['activation_202[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_94 (Concatenate)   (None, 375, 352)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_203 (Batch  (None, 375, 352)    1408        ['concatenate_94[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_203 (Activation)    (None, 375, 352)     0           ['batch_normalization_203[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_201 (Conv1D)            (None, 375, 128)     45184       ['activation_203[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_204 (Batch  (None, 375, 128)    512         ['conv1d_201[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_204 (Activation)    (None, 375, 128)     0           ['batch_normalization_204[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_202 (Conv1D)            (None, 375, 32)      12320       ['activation_204[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_95 (Concatenate)   (None, 375, 384)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_205 (Batch  (None, 375, 384)    1536        ['concatenate_95[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " activation_205 (Activation)    (None, 375, 384)     0           ['batch_normalization_205[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_203 (Conv1D)            (None, 375, 128)     49280       ['activation_205[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_206 (Batch  (None, 375, 128)    512         ['conv1d_203[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_206 (Activation)    (None, 375, 128)     0           ['batch_normalization_206[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_204 (Conv1D)            (None, 375, 32)      12320       ['activation_206[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_96 (Concatenate)   (None, 375, 416)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_207 (Batch  (None, 375, 416)    1664        ['concatenate_96[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_207 (Activation)    (None, 375, 416)     0           ['batch_normalization_207[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_205 (Conv1D)            (None, 375, 128)     53376       ['activation_207[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_208 (Batch  (None, 375, 128)    512         ['conv1d_205[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_208 (Activation)    (None, 375, 128)     0           ['batch_normalization_208[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_206 (Conv1D)            (None, 375, 32)      12320       ['activation_208[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_97 (Concatenate)   (None, 375, 448)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_209 (Batch  (None, 375, 448)    1792        ['concatenate_97[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_209 (Activation)    (None, 375, 448)     0           ['batch_normalization_209[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_207 (Conv1D)            (None, 375, 128)     57472       ['activation_209[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_210 (Batch  (None, 375, 128)    512         ['conv1d_207[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_210 (Activation)    (None, 375, 128)     0           ['batch_normalization_210[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_208 (Conv1D)            (None, 375, 32)      12320       ['activation_210[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_98 (Concatenate)   (None, 375, 480)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_211 (Batch  (None, 375, 480)    1920        ['concatenate_98[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_211 (Activation)    (None, 375, 480)     0           ['batch_normalization_211[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_209 (Conv1D)            (None, 375, 128)     61568       ['activation_211[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_212 (Batch  (None, 375, 128)    512         ['conv1d_209[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_212 (Activation)    (None, 375, 128)     0           ['batch_normalization_212[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_210 (Conv1D)            (None, 375, 32)      12320       ['activation_212[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_99 (Concatenate)   (None, 375, 512)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_213 (Batch  (None, 375, 512)    2048        ['concatenate_99[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_213 (Activation)    (None, 375, 512)     0           ['batch_normalization_213[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_211 (Conv1D)            (None, 375, 128)     65664       ['activation_213[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_214 (Batch  (None, 375, 128)    512         ['conv1d_211[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_214 (Activation)    (None, 375, 128)     0           ['batch_normalization_214[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_212 (Conv1D)            (None, 375, 32)      12320       ['activation_214[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_100 (Concatenate)  (None, 375, 544)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_215 (Batch  (None, 375, 544)    2176        ['concatenate_100[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_215 (Activation)    (None, 375, 544)     0           ['batch_normalization_215[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_213 (Conv1D)            (None, 375, 128)     69760       ['activation_215[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_216 (Batch  (None, 375, 128)    512         ['conv1d_213[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_216 (Activation)    (None, 375, 128)     0           ['batch_normalization_216[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_214 (Conv1D)            (None, 375, 32)      12320       ['activation_216[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_101 (Concatenate)  (None, 375, 576)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_217 (Batch  (None, 375, 576)    2304        ['concatenate_101[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_217 (Activation)    (None, 375, 576)     0           ['batch_normalization_217[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_215 (Conv1D)            (None, 375, 128)     73856       ['activation_217[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_218 (Batch  (None, 375, 128)    512         ['conv1d_215[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_218 (Activation)    (None, 375, 128)     0           ['batch_normalization_218[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_216 (Conv1D)            (None, 375, 32)      12320       ['activation_218[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_102 (Concatenate)  (None, 375, 608)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_219 (Batch  (None, 375, 608)    2432        ['concatenate_102[0][0]']        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_219 (Activation)    (None, 375, 608)     0           ['batch_normalization_219[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_217 (Conv1D)            (None, 375, 128)     77952       ['activation_219[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_220 (Batch  (None, 375, 128)    512         ['conv1d_217[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_220 (Activation)    (None, 375, 128)     0           ['batch_normalization_220[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_218 (Conv1D)            (None, 375, 32)      12320       ['activation_220[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_103 (Concatenate)  (None, 375, 640)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_221 (Batch  (None, 375, 640)    2560        ['concatenate_103[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_221 (Activation)    (None, 375, 640)     0           ['batch_normalization_221[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_219 (Conv1D)            (None, 375, 128)     82048       ['activation_221[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_222 (Batch  (None, 375, 128)    512         ['conv1d_219[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_222 (Activation)    (None, 375, 128)     0           ['batch_normalization_222[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_220 (Conv1D)            (None, 375, 32)      12320       ['activation_222[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_104 (Concatenate)  (None, 375, 672)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_223 (Batch  (None, 375, 672)    2688        ['concatenate_104[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_223 (Activation)    (None, 375, 672)     0           ['batch_normalization_223[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_221 (Conv1D)            (None, 375, 128)     86144       ['activation_223[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_224 (Batch  (None, 375, 128)    512         ['conv1d_221[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_224 (Activation)    (None, 375, 128)     0           ['batch_normalization_224[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_222 (Conv1D)            (None, 375, 32)      12320       ['activation_224[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_105 (Concatenate)  (None, 375, 704)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_225 (Batch  (None, 375, 704)    2816        ['concatenate_105[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_225 (Activation)    (None, 375, 704)     0           ['batch_normalization_225[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_223 (Conv1D)            (None, 375, 128)     90240       ['activation_225[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_226 (Batch  (None, 375, 128)    512         ['conv1d_223[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_226 (Activation)    (None, 375, 128)     0           ['batch_normalization_226[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_224 (Conv1D)            (None, 375, 32)      12320       ['activation_226[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_106 (Concatenate)  (None, 375, 736)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_227 (Batch  (None, 375, 736)    2944        ['concatenate_106[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_227 (Activation)    (None, 375, 736)     0           ['batch_normalization_227[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_225 (Conv1D)            (None, 375, 128)     94336       ['activation_227[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_228 (Batch  (None, 375, 128)    512         ['conv1d_225[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_228 (Activation)    (None, 375, 128)     0           ['batch_normalization_228[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_226 (Conv1D)            (None, 375, 32)      12320       ['activation_228[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_107 (Concatenate)  (None, 375, 768)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]',             \n",
      "                                                                  'conv1d_226[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_229 (Batch  (None, 375, 768)    3072        ['concatenate_107[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_229 (Activation)    (None, 375, 768)     0           ['batch_normalization_229[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_227 (Conv1D)            (None, 375, 128)     98432       ['activation_229[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_230 (Batch  (None, 375, 128)    512         ['conv1d_227[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_230 (Activation)    (None, 375, 128)     0           ['batch_normalization_230[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_228 (Conv1D)            (None, 375, 32)      12320       ['activation_230[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_108 (Concatenate)  (None, 375, 800)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]',             \n",
      "                                                                  'conv1d_226[0][0]',             \n",
      "                                                                  'conv1d_228[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_231 (Batch  (None, 375, 800)    3200        ['concatenate_108[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_231 (Activation)    (None, 375, 800)     0           ['batch_normalization_231[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_229 (Conv1D)            (None, 375, 128)     102528      ['activation_231[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_232 (Batch  (None, 375, 128)    512         ['conv1d_229[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_232 (Activation)    (None, 375, 128)     0           ['batch_normalization_232[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_230 (Conv1D)            (None, 375, 32)      12320       ['activation_232[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_109 (Concatenate)  (None, 375, 832)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]',             \n",
      "                                                                  'conv1d_226[0][0]',             \n",
      "                                                                  'conv1d_228[0][0]',             \n",
      "                                                                  'conv1d_230[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_233 (Batch  (None, 375, 832)    3328        ['concatenate_109[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_233 (Activation)    (None, 375, 832)     0           ['batch_normalization_233[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_231 (Conv1D)            (None, 375, 128)     106624      ['activation_233[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_234 (Batch  (None, 375, 128)    512         ['conv1d_231[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_234 (Activation)    (None, 375, 128)     0           ['batch_normalization_234[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_232 (Conv1D)            (None, 375, 32)      12320       ['activation_234[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_110 (Concatenate)  (None, 375, 864)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]',             \n",
      "                                                                  'conv1d_226[0][0]',             \n",
      "                                                                  'conv1d_228[0][0]',             \n",
      "                                                                  'conv1d_230[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv1d_232[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_235 (Batch  (None, 375, 864)    3456        ['concatenate_110[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_235 (Activation)    (None, 375, 864)     0           ['batch_normalization_235[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_233 (Conv1D)            (None, 375, 128)     110720      ['activation_235[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_236 (Batch  (None, 375, 128)    512         ['conv1d_233[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_236 (Activation)    (None, 375, 128)     0           ['batch_normalization_236[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_234 (Conv1D)            (None, 375, 32)      12320       ['activation_236[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_111 (Concatenate)  (None, 375, 896)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]',             \n",
      "                                                                  'conv1d_226[0][0]',             \n",
      "                                                                  'conv1d_228[0][0]',             \n",
      "                                                                  'conv1d_230[0][0]',             \n",
      "                                                                  'conv1d_232[0][0]',             \n",
      "                                                                  'conv1d_234[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_237 (Batch  (None, 375, 896)    3584        ['concatenate_111[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_237 (Activation)    (None, 375, 896)     0           ['batch_normalization_237[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_235 (Conv1D)            (None, 375, 128)     114816      ['activation_237[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_238 (Batch  (None, 375, 128)    512         ['conv1d_235[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_238 (Activation)    (None, 375, 128)     0           ['batch_normalization_238[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_236 (Conv1D)            (None, 375, 32)      12320       ['activation_238[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_112 (Concatenate)  (None, 375, 928)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]',             \n",
      "                                                                  'conv1d_226[0][0]',             \n",
      "                                                                  'conv1d_228[0][0]',             \n",
      "                                                                  'conv1d_230[0][0]',             \n",
      "                                                                  'conv1d_232[0][0]',             \n",
      "                                                                  'conv1d_234[0][0]',             \n",
      "                                                                  'conv1d_236[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_239 (Batch  (None, 375, 928)    3712        ['concatenate_112[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_239 (Activation)    (None, 375, 928)     0           ['batch_normalization_239[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_237 (Conv1D)            (None, 375, 128)     118912      ['activation_239[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_240 (Batch  (None, 375, 128)    512         ['conv1d_237[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_240 (Activation)    (None, 375, 128)     0           ['batch_normalization_240[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_238 (Conv1D)            (None, 375, 32)      12320       ['activation_240[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_113 (Concatenate)  (None, 375, 960)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]',             \n",
      "                                                                  'conv1d_226[0][0]',             \n",
      "                                                                  'conv1d_228[0][0]',             \n",
      "                                                                  'conv1d_230[0][0]',             \n",
      "                                                                  'conv1d_232[0][0]',             \n",
      "                                                                  'conv1d_234[0][0]',             \n",
      "                                                                  'conv1d_236[0][0]',             \n",
      "                                                                  'conv1d_238[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_241 (Batch  (None, 375, 960)    3840        ['concatenate_113[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_241 (Activation)    (None, 375, 960)     0           ['batch_normalization_241[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_239 (Conv1D)            (None, 375, 128)     123008      ['activation_241[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_242 (Batch  (None, 375, 128)    512         ['conv1d_239[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_242 (Activation)    (None, 375, 128)     0           ['batch_normalization_242[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_240 (Conv1D)            (None, 375, 32)      12320       ['activation_242[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_114 (Concatenate)  (None, 375, 992)     0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]',             \n",
      "                                                                  'conv1d_226[0][0]',             \n",
      "                                                                  'conv1d_228[0][0]',             \n",
      "                                                                  'conv1d_230[0][0]',             \n",
      "                                                                  'conv1d_232[0][0]',             \n",
      "                                                                  'conv1d_234[0][0]',             \n",
      "                                                                  'conv1d_236[0][0]',             \n",
      "                                                                  'conv1d_238[0][0]',             \n",
      "                                                                  'conv1d_240[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_243 (Batch  (None, 375, 992)    3968        ['concatenate_114[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_243 (Activation)    (None, 375, 992)     0           ['batch_normalization_243[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_241 (Conv1D)            (None, 375, 128)     127104      ['activation_243[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_244 (Batch  (None, 375, 128)    512         ['conv1d_241[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_244 (Activation)    (None, 375, 128)     0           ['batch_normalization_244[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_242 (Conv1D)            (None, 375, 32)      12320       ['activation_244[0][0]']         \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " concatenate_115 (Concatenate)  (None, 375, 1024)    0           ['average_pooling1d_7[0][0]',    \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_198[0][0]',             \n",
      "                                                                  'conv1d_200[0][0]',             \n",
      "                                                                  'conv1d_202[0][0]',             \n",
      "                                                                  'conv1d_204[0][0]',             \n",
      "                                                                  'conv1d_206[0][0]',             \n",
      "                                                                  'conv1d_208[0][0]',             \n",
      "                                                                  'conv1d_210[0][0]',             \n",
      "                                                                  'conv1d_212[0][0]',             \n",
      "                                                                  'conv1d_214[0][0]',             \n",
      "                                                                  'conv1d_216[0][0]',             \n",
      "                                                                  'conv1d_218[0][0]',             \n",
      "                                                                  'conv1d_220[0][0]',             \n",
      "                                                                  'conv1d_222[0][0]',             \n",
      "                                                                  'conv1d_224[0][0]',             \n",
      "                                                                  'conv1d_226[0][0]',             \n",
      "                                                                  'conv1d_228[0][0]',             \n",
      "                                                                  'conv1d_230[0][0]',             \n",
      "                                                                  'conv1d_232[0][0]',             \n",
      "                                                                  'conv1d_234[0][0]',             \n",
      "                                                                  'conv1d_236[0][0]',             \n",
      "                                                                  'conv1d_238[0][0]',             \n",
      "                                                                  'conv1d_240[0][0]',             \n",
      "                                                                  'conv1d_242[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_245 (Batch  (None, 375, 1024)   4096        ['concatenate_115[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_245 (Activation)    (None, 375, 1024)    0           ['batch_normalization_245[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_243 (Conv1D)            (None, 375, 512)     524800      ['activation_245[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling1d_8 (AveragePo  (None, 188, 512)    0           ['conv1d_243[0][0]']             \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_246 (Batch  (None, 188, 512)    2048        ['average_pooling1d_8[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_246 (Activation)    (None, 188, 512)     0           ['batch_normalization_246[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_244 (Conv1D)            (None, 188, 128)     65664       ['activation_246[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_247 (Batch  (None, 188, 128)    512         ['conv1d_244[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_247 (Activation)    (None, 188, 128)     0           ['batch_normalization_247[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_245 (Conv1D)            (None, 188, 32)      12320       ['activation_247[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_116 (Concatenate)  (None, 188, 544)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_248 (Batch  (None, 188, 544)    2176        ['concatenate_116[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_248 (Activation)    (None, 188, 544)     0           ['batch_normalization_248[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_246 (Conv1D)            (None, 188, 128)     69760       ['activation_248[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_249 (Batch  (None, 188, 128)    512         ['conv1d_246[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_249 (Activation)    (None, 188, 128)     0           ['batch_normalization_249[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_247 (Conv1D)            (None, 188, 32)      12320       ['activation_249[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_117 (Concatenate)  (None, 188, 576)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_250 (Batch  (None, 188, 576)    2304        ['concatenate_117[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_250 (Activation)    (None, 188, 576)     0           ['batch_normalization_250[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_248 (Conv1D)            (None, 188, 128)     73856       ['activation_250[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_251 (Batch  (None, 188, 128)    512         ['conv1d_248[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_251 (Activation)    (None, 188, 128)     0           ['batch_normalization_251[0][0]']\n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv1d_249 (Conv1D)            (None, 188, 32)      12320       ['activation_251[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_118 (Concatenate)  (None, 188, 608)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_252 (Batch  (None, 188, 608)    2432        ['concatenate_118[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_252 (Activation)    (None, 188, 608)     0           ['batch_normalization_252[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_250 (Conv1D)            (None, 188, 128)     77952       ['activation_252[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_253 (Batch  (None, 188, 128)    512         ['conv1d_250[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_253 (Activation)    (None, 188, 128)     0           ['batch_normalization_253[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_251 (Conv1D)            (None, 188, 32)      12320       ['activation_253[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_119 (Concatenate)  (None, 188, 640)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_254 (Batch  (None, 188, 640)    2560        ['concatenate_119[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_254 (Activation)    (None, 188, 640)     0           ['batch_normalization_254[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_252 (Conv1D)            (None, 188, 128)     82048       ['activation_254[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_255 (Batch  (None, 188, 128)    512         ['conv1d_252[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_255 (Activation)    (None, 188, 128)     0           ['batch_normalization_255[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_253 (Conv1D)            (None, 188, 32)      12320       ['activation_255[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_120 (Concatenate)  (None, 188, 672)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_256 (Batch  (None, 188, 672)    2688        ['concatenate_120[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_256 (Activation)    (None, 188, 672)     0           ['batch_normalization_256[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_254 (Conv1D)            (None, 188, 128)     86144       ['activation_256[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_257 (Batch  (None, 188, 128)    512         ['conv1d_254[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_257 (Activation)    (None, 188, 128)     0           ['batch_normalization_257[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_255 (Conv1D)            (None, 188, 32)      12320       ['activation_257[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_121 (Concatenate)  (None, 188, 704)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_258 (Batch  (None, 188, 704)    2816        ['concatenate_121[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_258 (Activation)    (None, 188, 704)     0           ['batch_normalization_258[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_256 (Conv1D)            (None, 188, 128)     90240       ['activation_258[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_259 (Batch  (None, 188, 128)    512         ['conv1d_256[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_259 (Activation)    (None, 188, 128)     0           ['batch_normalization_259[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_257 (Conv1D)            (None, 188, 32)      12320       ['activation_259[0][0]']         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " concatenate_122 (Concatenate)  (None, 188, 736)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_260 (Batch  (None, 188, 736)    2944        ['concatenate_122[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_260 (Activation)    (None, 188, 736)     0           ['batch_normalization_260[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_258 (Conv1D)            (None, 188, 128)     94336       ['activation_260[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_261 (Batch  (None, 188, 128)    512         ['conv1d_258[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_261 (Activation)    (None, 188, 128)     0           ['batch_normalization_261[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_259 (Conv1D)            (None, 188, 32)      12320       ['activation_261[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_123 (Concatenate)  (None, 188, 768)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]',             \n",
      "                                                                  'conv1d_259[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_262 (Batch  (None, 188, 768)    3072        ['concatenate_123[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_262 (Activation)    (None, 188, 768)     0           ['batch_normalization_262[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_260 (Conv1D)            (None, 188, 128)     98432       ['activation_262[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_263 (Batch  (None, 188, 128)    512         ['conv1d_260[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_263 (Activation)    (None, 188, 128)     0           ['batch_normalization_263[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_261 (Conv1D)            (None, 188, 32)      12320       ['activation_263[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_124 (Concatenate)  (None, 188, 800)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]',             \n",
      "                                                                  'conv1d_259[0][0]',             \n",
      "                                                                  'conv1d_261[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_264 (Batch  (None, 188, 800)    3200        ['concatenate_124[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_264 (Activation)    (None, 188, 800)     0           ['batch_normalization_264[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_262 (Conv1D)            (None, 188, 128)     102528      ['activation_264[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_265 (Batch  (None, 188, 128)    512         ['conv1d_262[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_265 (Activation)    (None, 188, 128)     0           ['batch_normalization_265[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_263 (Conv1D)            (None, 188, 32)      12320       ['activation_265[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_125 (Concatenate)  (None, 188, 832)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]',             \n",
      "                                                                  'conv1d_259[0][0]',             \n",
      "                                                                  'conv1d_261[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv1d_263[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_266 (Batch  (None, 188, 832)    3328        ['concatenate_125[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_266 (Activation)    (None, 188, 832)     0           ['batch_normalization_266[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_264 (Conv1D)            (None, 188, 128)     106624      ['activation_266[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_267 (Batch  (None, 188, 128)    512         ['conv1d_264[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_267 (Activation)    (None, 188, 128)     0           ['batch_normalization_267[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_265 (Conv1D)            (None, 188, 32)      12320       ['activation_267[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_126 (Concatenate)  (None, 188, 864)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]',             \n",
      "                                                                  'conv1d_259[0][0]',             \n",
      "                                                                  'conv1d_261[0][0]',             \n",
      "                                                                  'conv1d_263[0][0]',             \n",
      "                                                                  'conv1d_265[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_268 (Batch  (None, 188, 864)    3456        ['concatenate_126[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_268 (Activation)    (None, 188, 864)     0           ['batch_normalization_268[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_266 (Conv1D)            (None, 188, 128)     110720      ['activation_268[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_269 (Batch  (None, 188, 128)    512         ['conv1d_266[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_269 (Activation)    (None, 188, 128)     0           ['batch_normalization_269[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_267 (Conv1D)            (None, 188, 32)      12320       ['activation_269[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_127 (Concatenate)  (None, 188, 896)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]',             \n",
      "                                                                  'conv1d_259[0][0]',             \n",
      "                                                                  'conv1d_261[0][0]',             \n",
      "                                                                  'conv1d_263[0][0]',             \n",
      "                                                                  'conv1d_265[0][0]',             \n",
      "                                                                  'conv1d_267[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_270 (Batch  (None, 188, 896)    3584        ['concatenate_127[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_270 (Activation)    (None, 188, 896)     0           ['batch_normalization_270[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_268 (Conv1D)            (None, 188, 128)     114816      ['activation_270[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_271 (Batch  (None, 188, 128)    512         ['conv1d_268[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_271 (Activation)    (None, 188, 128)     0           ['batch_normalization_271[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_269 (Conv1D)            (None, 188, 32)      12320       ['activation_271[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_128 (Concatenate)  (None, 188, 928)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]',             \n",
      "                                                                  'conv1d_259[0][0]',             \n",
      "                                                                  'conv1d_261[0][0]',             \n",
      "                                                                  'conv1d_263[0][0]',             \n",
      "                                                                  'conv1d_265[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv1d_267[0][0]',             \n",
      "                                                                  'conv1d_269[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_272 (Batch  (None, 188, 928)    3712        ['concatenate_128[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_272 (Activation)    (None, 188, 928)     0           ['batch_normalization_272[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_270 (Conv1D)            (None, 188, 128)     118912      ['activation_272[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_273 (Batch  (None, 188, 128)    512         ['conv1d_270[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_273 (Activation)    (None, 188, 128)     0           ['batch_normalization_273[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_271 (Conv1D)            (None, 188, 32)      12320       ['activation_273[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_129 (Concatenate)  (None, 188, 960)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]',             \n",
      "                                                                  'conv1d_259[0][0]',             \n",
      "                                                                  'conv1d_261[0][0]',             \n",
      "                                                                  'conv1d_263[0][0]',             \n",
      "                                                                  'conv1d_265[0][0]',             \n",
      "                                                                  'conv1d_267[0][0]',             \n",
      "                                                                  'conv1d_269[0][0]',             \n",
      "                                                                  'conv1d_271[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_274 (Batch  (None, 188, 960)    3840        ['concatenate_129[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_274 (Activation)    (None, 188, 960)     0           ['batch_normalization_274[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_272 (Conv1D)            (None, 188, 128)     123008      ['activation_274[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_275 (Batch  (None, 188, 128)    512         ['conv1d_272[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_275 (Activation)    (None, 188, 128)     0           ['batch_normalization_275[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_273 (Conv1D)            (None, 188, 32)      12320       ['activation_275[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_130 (Concatenate)  (None, 188, 992)     0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n",
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]',             \n",
      "                                                                  'conv1d_259[0][0]',             \n",
      "                                                                  'conv1d_261[0][0]',             \n",
      "                                                                  'conv1d_263[0][0]',             \n",
      "                                                                  'conv1d_265[0][0]',             \n",
      "                                                                  'conv1d_267[0][0]',             \n",
      "                                                                  'conv1d_269[0][0]',             \n",
      "                                                                  'conv1d_271[0][0]',             \n",
      "                                                                  'conv1d_273[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_276 (Batch  (None, 188, 992)    3968        ['concatenate_130[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_276 (Activation)    (None, 188, 992)     0           ['batch_normalization_276[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_274 (Conv1D)            (None, 188, 128)     127104      ['activation_276[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_277 (Batch  (None, 188, 128)    512         ['conv1d_274[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_277 (Activation)    (None, 188, 128)     0           ['batch_normalization_277[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_275 (Conv1D)            (None, 188, 32)      12320       ['activation_277[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_131 (Concatenate)  (None, 188, 1024)    0           ['average_pooling1d_8[0][0]',    \n",
      "                                                                  'conv1d_245[0][0]',             \n",
      "                                                                  'conv1d_247[0][0]',             \n",
      "                                                                  'conv1d_249[0][0]',             \n",
      "                                                                  'conv1d_251[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv1d_253[0][0]',             \n",
      "                                                                  'conv1d_255[0][0]',             \n",
      "                                                                  'conv1d_257[0][0]',             \n",
      "                                                                  'conv1d_259[0][0]',             \n",
      "                                                                  'conv1d_261[0][0]',             \n",
      "                                                                  'conv1d_263[0][0]',             \n",
      "                                                                  'conv1d_265[0][0]',             \n",
      "                                                                  'conv1d_267[0][0]',             \n",
      "                                                                  'conv1d_269[0][0]',             \n",
      "                                                                  'conv1d_271[0][0]',             \n",
      "                                                                  'conv1d_273[0][0]',             \n",
      "                                                                  'conv1d_275[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_278 (Batch  (None, 188, 1024)   4096        ['concatenate_131[0][0]']        \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_278 (Activation)    (None, 188, 1024)    0           ['batch_normalization_278[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 1024)        0           ['activation_278[0][0]']         \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            2050        ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,616,322\n",
      "Trainable params: 5,532,674\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from densenet.classifiers.one_d import DenseNet121, DenseNetCustom\n",
    "model = DenseNet121(input_shape=(6000, 3))\n",
    "x = model.layers[-2].output\n",
    "\n",
    "# x = tf.keras.layers.Dense(64)(x)\n",
    "# x = tf.keras.layers.Dense(128)(x)\n",
    "# x = tf.keras.layers.Dense(256)(x)\n",
    "# x = tf.keras.layers.Dropout(.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(2)(x)\n",
    "new_model = tf.keras.Model(inputs=model.input, outputs=x)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a2d01e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_292 (Conv1D)         (None, 5996, 16)          256       \n",
      "                                                                 \n",
      " batch_normalization_295 (Ba  (None, 5996, 16)         64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_295 (Activation)  (None, 5996, 16)         0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 5996, 16)          0         \n",
      "                                                                 \n",
      " max_pooling1d_19 (MaxPoolin  (None, 2998, 16)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_293 (Conv1D)         (None, 2996, 32)          1568      \n",
      "                                                                 \n",
      " batch_normalization_296 (Ba  (None, 2996, 32)         128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_296 (Activation)  (None, 2996, 32)         0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 2996, 32)          0         \n",
      "                                                                 \n",
      " max_pooling1d_20 (MaxPoolin  (None, 1498, 32)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_294 (Conv1D)         (None, 1498, 64)          2112      \n",
      "                                                                 \n",
      " batch_normalization_297 (Ba  (None, 1498, 64)         256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_297 (Activation)  (None, 1498, 64)         0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 1498, 64)          0         \n",
      "                                                                 \n",
      " max_pooling1d_21 (MaxPoolin  (None, 749, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_295 (Conv1D)         (None, 749, 128)          8320      \n",
      "                                                                 \n",
      " batch_normalization_298 (Ba  (None, 749, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_298 (Activation)  (None, 749, 128)         0         \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 749, 128)          0         \n",
      "                                                                 \n",
      " max_pooling1d_22 (MaxPoolin  (None, 374, 128)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 47872)             0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 32)                1531936   \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,589,122\n",
      "Trainable params: 1,588,642\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class F1_Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.f1 = self.add_weight(name='f1', initializer='zeros')\n",
    "        self.precision_fn = tf.keras.metrics.Precision(thresholds=0.5)\n",
    "        self.recall_fn = tf.keras.metrics.Recall(thresholds=0.5)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        p = self.precision_fn(y_true, y_pred)\n",
    "        r = self.recall_fn(y_true, y_pred)\n",
    "        self.f1.assign(2 *((p*r)/(p + r + 1e-6)))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.f1\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision_fn.reset_states()\n",
    "        self.recall_fn.reset_states()\n",
    "        self.f1.assign(0)\n",
    "\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.Input((6000, 3)),\n",
    "    \n",
    "    tf.keras.layers.Conv1D(16, kernel_size=5),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.MaxPooling1D(),\n",
    "    \n",
    "    tf.keras.layers.Conv1D(32, kernel_size=3),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.MaxPooling1D(),\n",
    "    \n",
    "    tf.keras.layers.Conv1D(64, kernel_size=1),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.MaxPooling1D(),\n",
    "    \n",
    "    tf.keras.layers.Conv1D(128, kernel_size=1),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.MaxPooling1D(),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(32),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(256),\n",
    "\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    " \n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model_2.compile(optimizer=final_opt, loss=tf.keras.losses.MeanSquaredError())\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7941bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epoch 0 *****\n",
      "Memory usage on epoch begin: 32.95GB\n",
      "Epoch 1/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 5.5138 - Memory usage on epoch end: 33.02GB\n",
      "1262/1262 [==============================] - 124s 96ms/step - loss: 5.5138 - val_loss: 0.5044 - lr: 0.0010\n",
      "***** Epoch 1 *****\n",
      "Memory usage on epoch begin: 33.02GB\n",
      "Epoch 2/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.6343 - Memory usage on epoch end: 33.02GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.6343 - val_loss: 0.5000 - lr: 0.0010\n",
      "***** Epoch 2 *****\n",
      "Memory usage on epoch begin: 33.02GB\n",
      "Epoch 3/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.5995 - Memory usage on epoch end: 33.00GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.5995 - val_loss: 0.5001 - lr: 0.0010\n",
      "***** Epoch 3 *****\n",
      "Memory usage on epoch begin: 33.00GB\n",
      "Epoch 4/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.5458 - Memory usage on epoch end: 32.98GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.5458 - val_loss: 0.5008 - lr: 0.0010\n",
      "***** Epoch 4 *****\n",
      "Memory usage on epoch begin: 32.98GB\n",
      "Epoch 5/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.5290 - Memory usage on epoch end: 32.96GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.5290 - val_loss: 0.5002 - lr: 0.0010\n",
      "***** Epoch 5 *****\n",
      "Memory usage on epoch begin: 32.96GB\n",
      "Epoch 6/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.5204 - Memory usage on epoch end: 32.95GB\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.5204 - val_loss: 0.5001 - lr: 0.0010\n",
      "***** Epoch 6 *****\n",
      "Memory usage on epoch begin: 32.95GB\n",
      "Epoch 7/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.5074 - Memory usage on epoch end: 32.94GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.5074 - val_loss: 0.5005 - lr: 8.0000e-04\n",
      "***** Epoch 7 *****\n",
      "Memory usage on epoch begin: 32.94GB\n",
      "Epoch 8/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.5038 - Memory usage on epoch end: 32.93GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.5038 - val_loss: 0.5001 - lr: 8.0000e-04\n",
      "***** Epoch 8 *****\n",
      "Memory usage on epoch begin: 32.93GB\n",
      "Epoch 9/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.5001 - Memory usage on epoch end: 32.92GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.5001 - val_loss: 0.5004 - lr: 8.0000e-04\n",
      "***** Epoch 9 *****\n",
      "Memory usage on epoch begin: 32.92GB\n",
      "Epoch 10/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4972 - Memory usage on epoch end: 32.91GB\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4972 - val_loss: 0.5012 - lr: 8.0000e-04\n",
      "***** Epoch 10 *****\n",
      "Memory usage on epoch begin: 32.91GB\n",
      "Epoch 11/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4952 - Memory usage on epoch end: 32.90GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4952 - val_loss: 0.5029 - lr: 6.4000e-04\n",
      "***** Epoch 11 *****\n",
      "Memory usage on epoch begin: 32.90GB\n",
      "Epoch 12/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4935 - Memory usage on epoch end: 32.89GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4935 - val_loss: 0.5073 - lr: 6.4000e-04\n",
      "***** Epoch 12 *****\n",
      "Memory usage on epoch begin: 32.89GB\n",
      "Epoch 13/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4914 - Memory usage on epoch end: 32.88GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4914 - val_loss: 0.5003 - lr: 6.4000e-04\n",
      "***** Epoch 13 *****\n",
      "Memory usage on epoch begin: 32.88GB\n",
      "Epoch 14/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4888 - Memory usage on epoch end: 32.87GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4888 - val_loss: 0.4954 - lr: 6.4000e-04\n",
      "***** Epoch 14 *****\n",
      "Memory usage on epoch begin: 32.87GB\n",
      "Epoch 15/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4872 - Memory usage on epoch end: 32.86GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4872 - val_loss: 0.5010 - lr: 6.4000e-04\n",
      "***** Epoch 15 *****\n",
      "Memory usage on epoch begin: 32.85GB\n",
      "Epoch 16/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4850 - Memory usage on epoch end: 32.85GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4850 - val_loss: 0.4971 - lr: 6.4000e-04\n",
      "***** Epoch 16 *****\n",
      "Memory usage on epoch begin: 32.85GB\n",
      "Epoch 17/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4835 - Memory usage on epoch end: 32.84GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4835 - val_loss: 0.5037 - lr: 6.4000e-04\n",
      "***** Epoch 17 *****\n",
      "Memory usage on epoch begin: 32.84GB\n",
      "Epoch 18/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4826 - Memory usage on epoch end: 32.83GB\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4826 - val_loss: 0.5030 - lr: 6.4000e-04\n",
      "***** Epoch 18 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 19/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4798 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4798 - val_loss: 0.5097 - lr: 5.1200e-04\n",
      "***** Epoch 19 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 20/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4784 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4784 - val_loss: 0.4975 - lr: 5.1200e-04\n",
      "***** Epoch 20 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 21/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4781 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4781 - val_loss: 0.4933 - lr: 5.1200e-04\n",
      "***** Epoch 21 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 22/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4764 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4764 - val_loss: 0.5034 - lr: 5.1200e-04\n",
      "***** Epoch 22 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 23/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4761 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4761 - val_loss: 0.5017 - lr: 5.1200e-04\n",
      "***** Epoch 23 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 24/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4747 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4747 - val_loss: 0.5019 - lr: 5.1200e-04\n",
      "***** Epoch 24 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 25/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4734 - Memory usage on epoch end: 32.77GB\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4734 - val_loss: 0.4937 - lr: 5.1200e-04\n",
      "***** Epoch 25 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 26/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4717 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4717 - val_loss: 0.5067 - lr: 4.0960e-04\n",
      "***** Epoch 26 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 27/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4708 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4708 - val_loss: 0.4990 - lr: 4.0960e-04\n",
      "***** Epoch 27 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 28/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4700 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4700 - val_loss: 0.5031 - lr: 4.0960e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epoch 28 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 29/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4701 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4701 - val_loss: 0.4926 - lr: 4.0960e-04\n",
      "***** Epoch 29 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 30/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4688 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4688 - val_loss: 0.5052 - lr: 4.0960e-04\n",
      "***** Epoch 30 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 31/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4678 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4678 - val_loss: 0.5060 - lr: 4.0960e-04\n",
      "***** Epoch 31 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 32/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4668 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4668 - val_loss: 0.4942 - lr: 4.0960e-04\n",
      "***** Epoch 32 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 33/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4671 - Memory usage on epoch end: 32.73GB\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4671 - val_loss: 0.4987 - lr: 4.0960e-04\n",
      "***** Epoch 33 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 34/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4651 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4651 - val_loss: 0.5073 - lr: 3.2768e-04\n",
      "***** Epoch 34 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 35/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4646 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4646 - val_loss: 0.5031 - lr: 3.2768e-04\n",
      "***** Epoch 35 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 36/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4637 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4637 - val_loss: 0.5021 - lr: 3.2768e-04\n",
      "***** Epoch 36 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 37/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4630 - Memory usage on epoch end: 32.73GB\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4630 - val_loss: 0.4978 - lr: 3.2768e-04\n",
      "***** Epoch 37 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 38/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4627 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4627 - val_loss: 0.5040 - lr: 2.6214e-04\n",
      "***** Epoch 38 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 39/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4616 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4616 - val_loss: 0.5010 - lr: 2.6214e-04\n",
      "***** Epoch 39 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 40/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4617 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4617 - val_loss: 0.5040 - lr: 2.6214e-04\n",
      "***** Epoch 40 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 41/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4609 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4609 - val_loss: 0.4894 - lr: 2.6214e-04\n",
      "***** Epoch 41 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 42/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4601 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4601 - val_loss: 0.4909 - lr: 2.6214e-04\n",
      "***** Epoch 42 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 43/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4606 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4606 - val_loss: 0.4971 - lr: 2.6214e-04\n",
      "***** Epoch 43 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 44/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4593 - Memory usage on epoch end: 32.73GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4593 - val_loss: 0.5018 - lr: 2.6214e-04\n",
      "***** Epoch 44 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 45/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4592 - Memory usage on epoch end: 32.73GB\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4592 - val_loss: 0.5052 - lr: 2.6214e-04\n",
      "***** Epoch 45 *****\n",
      "Memory usage on epoch begin: 32.73GB\n",
      "Epoch 46/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4584 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4584 - val_loss: 0.4946 - lr: 2.0972e-04\n",
      "***** Epoch 46 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 47/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4577 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4577 - val_loss: 0.4873 - lr: 2.0972e-04\n",
      "***** Epoch 47 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 48/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4575 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4575 - val_loss: 0.4909 - lr: 2.0972e-04\n",
      "***** Epoch 48 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 49/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4570 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4570 - val_loss: 0.4872 - lr: 2.0972e-04\n",
      "***** Epoch 49 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 50/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4569 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4569 - val_loss: 0.4906 - lr: 2.0972e-04\n",
      "***** Epoch 50 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 51/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4565 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4565 - val_loss: 0.4916 - lr: 2.0972e-04\n",
      "***** Epoch 51 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 52/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4565 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4565 - val_loss: 0.4933 - lr: 2.0972e-04\n",
      "***** Epoch 52 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 53/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4562 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4562 - val_loss: 0.4843 - lr: 2.0972e-04\n",
      "***** Epoch 53 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 54/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4564 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4564 - val_loss: 0.4982 - lr: 2.0972e-04\n",
      "***** Epoch 54 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 55/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4554 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4554 - val_loss: 0.5008 - lr: 2.0972e-04\n",
      "***** Epoch 55 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 56/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4550 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4550 - val_loss: 0.4991 - lr: 2.0972e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epoch 56 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 57/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4550 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4550 - val_loss: 0.4832 - lr: 2.0972e-04\n",
      "***** Epoch 57 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 58/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4544 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4544 - val_loss: 0.4895 - lr: 2.0972e-04\n",
      "***** Epoch 58 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 59/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4549 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4549 - val_loss: 0.4897 - lr: 2.0972e-04\n",
      "***** Epoch 59 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 60/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4537 - Memory usage on epoch end: 32.74GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4537 - val_loss: 0.4876 - lr: 2.0972e-04\n",
      "***** Epoch 60 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 61/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4536 - Memory usage on epoch end: 32.74GB\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "1262/1262 [==============================] - 120s 95ms/step - loss: 0.4536 - val_loss: 0.4922 - lr: 2.0972e-04\n",
      "***** Epoch 61 *****\n",
      "Memory usage on epoch begin: 32.74GB\n",
      "Epoch 62/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4530 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4530 - val_loss: 0.4896 - lr: 1.6777e-04\n",
      "***** Epoch 62 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 63/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4533 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4533 - val_loss: 0.4875 - lr: 1.6777e-04\n",
      "***** Epoch 63 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 64/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4528 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4528 - val_loss: 0.4860 - lr: 1.6777e-04\n",
      "***** Epoch 64 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 65/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4523 - Memory usage on epoch end: 32.75GB\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4523 - val_loss: 0.4979 - lr: 1.6777e-04\n",
      "***** Epoch 65 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 66/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4517 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 120s 95ms/step - loss: 0.4517 - val_loss: 0.4931 - lr: 1.3422e-04\n",
      "***** Epoch 66 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 67/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4516 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4516 - val_loss: 0.4869 - lr: 1.3422e-04\n",
      "***** Epoch 67 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 68/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4520 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4520 - val_loss: 0.4980 - lr: 1.3422e-04\n",
      "***** Epoch 68 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 69/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4509 - Memory usage on epoch end: 32.75GB\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "1262/1262 [==============================] - 121s 95ms/step - loss: 0.4509 - val_loss: 0.4905 - lr: 1.3422e-04\n",
      "***** Epoch 69 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 70/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4507 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 120s 95ms/step - loss: 0.4507 - val_loss: 0.4885 - lr: 1.0737e-04\n",
      "***** Epoch 70 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 71/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4505 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 120s 95ms/step - loss: 0.4505 - val_loss: 0.4849 - lr: 1.0737e-04\n",
      "***** Epoch 71 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 72/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4502 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4502 - val_loss: 0.4925 - lr: 1.0737e-04\n",
      "***** Epoch 72 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 73/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4499 - Memory usage on epoch end: 32.75GB\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4499 - val_loss: 0.4861 - lr: 1.0737e-04\n",
      "***** Epoch 73 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 74/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4495 - Memory usage on epoch end: 32.75GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4495 - val_loss: 0.4912 - lr: 1.0000e-04\n",
      "***** Epoch 74 *****\n",
      "Memory usage on epoch begin: 32.75GB\n",
      "Epoch 75/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4498 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4498 - val_loss: 0.4925 - lr: 1.0000e-04\n",
      "***** Epoch 75 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 76/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4497 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4497 - val_loss: 0.4970 - lr: 1.0000e-04\n",
      "***** Epoch 76 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 77/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4495 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4495 - val_loss: 0.4987 - lr: 1.0000e-04\n",
      "***** Epoch 77 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 78/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4496 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4496 - val_loss: 0.4990 - lr: 1.0000e-04\n",
      "***** Epoch 78 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 79/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4489 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4489 - val_loss: 0.4887 - lr: 1.0000e-04\n",
      "***** Epoch 79 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 80/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4493 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4493 - val_loss: 0.4972 - lr: 1.0000e-04\n",
      "***** Epoch 80 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 81/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4486 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4486 - val_loss: 0.5004 - lr: 1.0000e-04\n",
      "***** Epoch 81 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 82/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4490 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4490 - val_loss: 0.4949 - lr: 1.0000e-04\n",
      "***** Epoch 82 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 83/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4489 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4489 - val_loss: 0.4910 - lr: 1.0000e-04\n",
      "***** Epoch 83 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 84/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4489 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4489 - val_loss: 0.4918 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epoch 84 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 85/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4482 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4482 - val_loss: 0.4925 - lr: 1.0000e-04\n",
      "***** Epoch 85 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 86/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4482 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4482 - val_loss: 0.4909 - lr: 1.0000e-04\n",
      "***** Epoch 86 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 87/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4481 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4481 - val_loss: 0.4920 - lr: 1.0000e-04\n",
      "***** Epoch 87 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 88/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4482 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4482 - val_loss: 0.4995 - lr: 1.0000e-04\n",
      "***** Epoch 88 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 89/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4479 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4479 - val_loss: 0.4851 - lr: 1.0000e-04\n",
      "***** Epoch 89 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 90/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4480 - Memory usage on epoch end: 32.76GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4480 - val_loss: 0.4914 - lr: 1.0000e-04\n",
      "***** Epoch 90 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 91/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4475 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4475 - val_loss: 0.4928 - lr: 1.0000e-04\n",
      "***** Epoch 91 *****\n",
      "Memory usage on epoch begin: 32.76GB\n",
      "Epoch 92/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4477 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4477 - val_loss: 0.4980 - lr: 1.0000e-04\n",
      "***** Epoch 92 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 93/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4471 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4471 - val_loss: 0.4927 - lr: 1.0000e-04\n",
      "***** Epoch 93 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 94/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4473 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4473 - val_loss: 0.4887 - lr: 1.0000e-04\n",
      "***** Epoch 94 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 95/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4473 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4473 - val_loss: 0.4931 - lr: 1.0000e-04\n",
      "***** Epoch 95 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 96/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4474 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4474 - val_loss: 0.4932 - lr: 1.0000e-04\n",
      "***** Epoch 96 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 97/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4464 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4464 - val_loss: 0.4866 - lr: 1.0000e-04\n",
      "***** Epoch 97 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 98/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4470 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4470 - val_loss: 0.4953 - lr: 1.0000e-04\n",
      "***** Epoch 98 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 99/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4468 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4468 - val_loss: 0.4867 - lr: 1.0000e-04\n",
      "***** Epoch 99 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 100/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4474 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4474 - val_loss: 0.4909 - lr: 1.0000e-04\n",
      "***** Epoch 100 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 101/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4461 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4461 - val_loss: 0.4873 - lr: 1.0000e-04\n",
      "***** Epoch 101 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 102/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4466 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4466 - val_loss: 0.4882 - lr: 1.0000e-04\n",
      "***** Epoch 102 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 103/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4465 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4465 - val_loss: 0.4972 - lr: 1.0000e-04\n",
      "***** Epoch 103 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 104/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4466 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4466 - val_loss: 0.4977 - lr: 1.0000e-04\n",
      "***** Epoch 104 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 105/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4460 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4460 - val_loss: 0.4994 - lr: 1.0000e-04\n",
      "***** Epoch 105 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 106/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4460 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4460 - val_loss: 0.4831 - lr: 1.0000e-04\n",
      "***** Epoch 106 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 107/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4463 - Memory usage on epoch end: 32.77GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4463 - val_loss: 0.4864 - lr: 1.0000e-04\n",
      "***** Epoch 107 *****\n",
      "Memory usage on epoch begin: 32.77GB\n",
      "Epoch 108/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4459 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4459 - val_loss: 0.5006 - lr: 1.0000e-04\n",
      "***** Epoch 108 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 109/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4453 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4453 - val_loss: 0.4870 - lr: 1.0000e-04\n",
      "***** Epoch 109 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 110/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4456 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4456 - val_loss: 0.4883 - lr: 1.0000e-04\n",
      "***** Epoch 110 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 111/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4455 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4455 - val_loss: 0.4914 - lr: 1.0000e-04\n",
      "***** Epoch 111 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 112/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4454 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4454 - val_loss: 0.4903 - lr: 1.0000e-04\n",
      "***** Epoch 112 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4453 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4453 - val_loss: 0.4998 - lr: 1.0000e-04\n",
      "***** Epoch 113 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 114/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4454 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4454 - val_loss: 0.4956 - lr: 1.0000e-04\n",
      "***** Epoch 114 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 115/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4451 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4451 - val_loss: 0.4883 - lr: 1.0000e-04\n",
      "***** Epoch 115 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 116/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4450 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4450 - val_loss: 0.4897 - lr: 1.0000e-04\n",
      "***** Epoch 116 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 117/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4446 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4446 - val_loss: 0.5022 - lr: 1.0000e-04\n",
      "***** Epoch 117 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 118/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4452 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4452 - val_loss: 0.5006 - lr: 1.0000e-04\n",
      "***** Epoch 118 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 119/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4448 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4448 - val_loss: 0.4877 - lr: 1.0000e-04\n",
      "***** Epoch 119 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 120/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4449 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4449 - val_loss: 0.4868 - lr: 1.0000e-04\n",
      "***** Epoch 120 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 121/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4446 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4446 - val_loss: 0.4870 - lr: 1.0000e-04\n",
      "***** Epoch 121 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 122/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4447 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4447 - val_loss: 0.4864 - lr: 1.0000e-04\n",
      "***** Epoch 122 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 123/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4450 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4450 - val_loss: 0.4891 - lr: 1.0000e-04\n",
      "***** Epoch 123 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 124/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4443 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4443 - val_loss: 0.4981 - lr: 1.0000e-04\n",
      "***** Epoch 124 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 125/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4440 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4440 - val_loss: 0.4869 - lr: 1.0000e-04\n",
      "***** Epoch 125 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 126/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4438 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4438 - val_loss: 0.4979 - lr: 1.0000e-04\n",
      "***** Epoch 126 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 127/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4446 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4446 - val_loss: 0.4896 - lr: 1.0000e-04\n",
      "***** Epoch 127 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 128/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4442 - Memory usage on epoch end: 32.78GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4442 - val_loss: 0.4956 - lr: 1.0000e-04\n",
      "***** Epoch 128 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 129/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4443 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4443 - val_loss: 0.5005 - lr: 1.0000e-04\n",
      "***** Epoch 129 *****\n",
      "Memory usage on epoch begin: 32.78GB\n",
      "Epoch 130/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4442 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4442 - val_loss: 0.5043 - lr: 1.0000e-04\n",
      "***** Epoch 130 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 131/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4434 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4434 - val_loss: 0.4989 - lr: 1.0000e-04\n",
      "***** Epoch 131 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 132/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4435 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4435 - val_loss: 0.4917 - lr: 1.0000e-04\n",
      "***** Epoch 132 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 133/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4436 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4436 - val_loss: 0.4917 - lr: 1.0000e-04\n",
      "***** Epoch 133 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 134/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4435 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4435 - val_loss: 0.4867 - lr: 1.0000e-04\n",
      "***** Epoch 134 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 135/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4432 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4432 - val_loss: 0.4929 - lr: 1.0000e-04\n",
      "***** Epoch 135 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 136/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4432 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4432 - val_loss: 0.4956 - lr: 1.0000e-04\n",
      "***** Epoch 136 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 137/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4433 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4433 - val_loss: 0.4949 - lr: 1.0000e-04\n",
      "***** Epoch 137 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 138/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4432 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4432 - val_loss: 0.4971 - lr: 1.0000e-04\n",
      "***** Epoch 138 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 139/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4432 - Memory usage on epoch end: 32.79GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4432 - val_loss: 0.5035 - lr: 1.0000e-04\n",
      "***** Epoch 139 *****\n",
      "Memory usage on epoch begin: 32.79GB\n",
      "Epoch 140/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4429 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4429 - val_loss: 0.5011 - lr: 1.0000e-04\n",
      "***** Epoch 140 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 141/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4430 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4430 - val_loss: 0.4949 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epoch 141 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 142/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4430 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4430 - val_loss: 0.5025 - lr: 1.0000e-04\n",
      "***** Epoch 142 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 143/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4425 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4425 - val_loss: 0.5028 - lr: 1.0000e-04\n",
      "***** Epoch 143 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 144/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4426 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4426 - val_loss: 0.4981 - lr: 1.0000e-04\n",
      "***** Epoch 144 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 145/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4430 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4430 - val_loss: 0.4943 - lr: 1.0000e-04\n",
      "***** Epoch 145 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 146/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4424 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4424 - val_loss: 0.4999 - lr: 1.0000e-04\n",
      "***** Epoch 146 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 147/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4425 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4425 - val_loss: 0.4984 - lr: 1.0000e-04\n",
      "***** Epoch 147 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 148/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4424 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4424 - val_loss: 0.5052 - lr: 1.0000e-04\n",
      "***** Epoch 148 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 149/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4423 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4423 - val_loss: 0.5031 - lr: 1.0000e-04\n",
      "***** Epoch 149 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 150/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4421 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4421 - val_loss: 0.4930 - lr: 1.0000e-04\n",
      "***** Epoch 150 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 151/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4419 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4419 - val_loss: 0.4913 - lr: 1.0000e-04\n",
      "***** Epoch 151 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 152/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4418 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4418 - val_loss: 0.4866 - lr: 1.0000e-04\n",
      "***** Epoch 152 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 153/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4420 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4420 - val_loss: 0.5041 - lr: 1.0000e-04\n",
      "***** Epoch 153 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 154/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4417 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4417 - val_loss: 0.5003 - lr: 1.0000e-04\n",
      "***** Epoch 154 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 155/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4417 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4417 - val_loss: 0.4894 - lr: 1.0000e-04\n",
      "***** Epoch 155 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 156/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4417 - Memory usage on epoch end: 32.80GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4417 - val_loss: 0.5010 - lr: 1.0000e-04\n",
      "***** Epoch 156 *****\n",
      "Memory usage on epoch begin: 32.80GB\n",
      "Epoch 157/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4418 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4418 - val_loss: 0.5014 - lr: 1.0000e-04\n",
      "***** Epoch 157 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 158/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4414 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4414 - val_loss: 0.4998 - lr: 1.0000e-04\n",
      "***** Epoch 158 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 159/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4415 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4415 - val_loss: 0.4916 - lr: 1.0000e-04\n",
      "***** Epoch 159 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 160/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4415 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4415 - val_loss: 0.5036 - lr: 1.0000e-04\n",
      "***** Epoch 160 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 161/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4414 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4414 - val_loss: 0.4973 - lr: 1.0000e-04\n",
      "***** Epoch 161 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 162/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4416 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4416 - val_loss: 0.5014 - lr: 1.0000e-04\n",
      "***** Epoch 162 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 163/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4410 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4410 - val_loss: 0.5011 - lr: 1.0000e-04\n",
      "***** Epoch 163 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 164/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4412 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4412 - val_loss: 0.5076 - lr: 1.0000e-04\n",
      "***** Epoch 164 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 165/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4414 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4414 - val_loss: 0.5064 - lr: 1.0000e-04\n",
      "***** Epoch 165 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 166/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4409 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4409 - val_loss: 0.5061 - lr: 1.0000e-04\n",
      "***** Epoch 166 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 167/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4408 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4408 - val_loss: 0.5015 - lr: 1.0000e-04\n",
      "***** Epoch 167 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 168/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4414 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4414 - val_loss: 0.5085 - lr: 1.0000e-04\n",
      "***** Epoch 168 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 169/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4408 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4408 - val_loss: 0.5013 - lr: 1.0000e-04\n",
      "***** Epoch 169 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4411 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4411 - val_loss: 0.4942 - lr: 1.0000e-04\n",
      "***** Epoch 170 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 171/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4408 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4408 - val_loss: 0.4984 - lr: 1.0000e-04\n",
      "***** Epoch 171 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 172/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4405 - Memory usage on epoch end: 32.81GB\n",
      "1262/1262 [==============================] - 122s 97ms/step - loss: 0.4405 - val_loss: 0.5055 - lr: 1.0000e-04\n",
      "***** Epoch 172 *****\n",
      "Memory usage on epoch begin: 32.81GB\n",
      "Epoch 173/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4406 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4406 - val_loss: 0.5090 - lr: 1.0000e-04\n",
      "***** Epoch 173 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 174/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4408 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4408 - val_loss: 0.4985 - lr: 1.0000e-04\n",
      "***** Epoch 174 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 175/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4402 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4402 - val_loss: 0.5094 - lr: 1.0000e-04\n",
      "***** Epoch 175 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 176/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4403 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4403 - val_loss: 0.4985 - lr: 1.0000e-04\n",
      "***** Epoch 176 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 177/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4402 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4402 - val_loss: 0.5065 - lr: 1.0000e-04\n",
      "***** Epoch 177 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 178/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4404 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4404 - val_loss: 0.5007 - lr: 1.0000e-04\n",
      "***** Epoch 178 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 179/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4405 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4405 - val_loss: 0.4954 - lr: 1.0000e-04\n",
      "***** Epoch 179 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 180/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4401 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4401 - val_loss: 0.4883 - lr: 1.0000e-04\n",
      "***** Epoch 180 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 181/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4403 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4403 - val_loss: 0.4936 - lr: 1.0000e-04\n",
      "***** Epoch 181 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 182/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4405 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4405 - val_loss: 0.5044 - lr: 1.0000e-04\n",
      "***** Epoch 182 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 183/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4400 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4400 - val_loss: 0.4980 - lr: 1.0000e-04\n",
      "***** Epoch 183 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 184/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4400 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4400 - val_loss: 0.5073 - lr: 1.0000e-04\n",
      "***** Epoch 184 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 185/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4398 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4398 - val_loss: 0.5003 - lr: 1.0000e-04\n",
      "***** Epoch 185 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 186/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4400 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4400 - val_loss: 0.4909 - lr: 1.0000e-04\n",
      "***** Epoch 186 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 187/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4399 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4399 - val_loss: 0.5048 - lr: 1.0000e-04\n",
      "***** Epoch 187 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 188/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4399 - Memory usage on epoch end: 32.82GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4399 - val_loss: 0.5075 - lr: 1.0000e-04\n",
      "***** Epoch 188 *****\n",
      "Memory usage on epoch begin: 32.82GB\n",
      "Epoch 189/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4394 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4394 - val_loss: 0.4979 - lr: 1.0000e-04\n",
      "***** Epoch 189 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 190/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4402 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4402 - val_loss: 0.5004 - lr: 1.0000e-04\n",
      "***** Epoch 190 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 191/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4394 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4394 - val_loss: 0.4934 - lr: 1.0000e-04\n",
      "***** Epoch 191 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 192/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4393 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4393 - val_loss: 0.5068 - lr: 1.0000e-04\n",
      "***** Epoch 192 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 193/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4396 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4396 - val_loss: 0.5053 - lr: 1.0000e-04\n",
      "***** Epoch 193 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 194/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4392 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4392 - val_loss: 0.4960 - lr: 1.0000e-04\n",
      "***** Epoch 194 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 195/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4395 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4395 - val_loss: 0.4918 - lr: 1.0000e-04\n",
      "***** Epoch 195 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 196/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4388 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4388 - val_loss: 0.4941 - lr: 1.0000e-04\n",
      "***** Epoch 196 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 197/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4389 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4389 - val_loss: 0.5023 - lr: 1.0000e-04\n",
      "***** Epoch 197 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 198/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4388 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 121s 96ms/step - loss: 0.4388 - val_loss: 0.5076 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epoch 198 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 199/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4391 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4391 - val_loss: 0.4951 - lr: 1.0000e-04\n",
      "***** Epoch 199 *****\n",
      "Memory usage on epoch begin: 32.83GB\n",
      "Epoch 200/200\n",
      "1262/1262 [==============================] - ETA: 0s - loss: 0.4385 - Memory usage on epoch end: 32.83GB\n",
      "1262/1262 [==============================] - 122s 96ms/step - loss: 0.4385 - val_loss: 0.5068 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=12)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8,\n",
    "                                                 patience=4, min_lr=0.0001, verbose=1)\n",
    "# checkpoint_filepath_epoch = os.path.join(os.getcwd(), 'Model Checkpoint', 'v5 (S-Wave 1)',\n",
    "#                                          'Model Checkpoint Every 2 Epoch')\n",
    "checkpoint_filepath_best = os.path.join(os.getcwd(), 'Model Checkpoint', 'v2 (Back Azimuth)',\n",
    "                                         'Model Checkpoint Best', 'Model_Epoch_{epoch}')\n",
    "model_checkpoint_callback_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_best,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "# model_checkpoint_callback_epoch = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_filepath_epoch,\n",
    "#     save_weights_only=False,\n",
    "#     save_freq=2*dt_train.batch_size)\n",
    "\n",
    "def GetHumanReadable(size,precision=2):\n",
    "    suffixes=['B','KB','MB','GB','TB']\n",
    "    suffixIndex = 0\n",
    "    while size >= 1024 and suffixIndex < 4:\n",
    "        suffixIndex += 1 #increment the index of the suffix\n",
    "        size = size/1024.0 #apply the division\n",
    "        \n",
    "    return \"%.*f%s\"%(precision,size,suffixes[suffixIndex])\n",
    "\n",
    "class MemoryUsageCallback(tf.keras.callbacks.Callback):\n",
    "    '''Monitor memory usage on epoch begin and end.'''\n",
    "\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "        print('***** Epoch {} *****'.format(epoch))\n",
    "        print('Memory usage on epoch begin: {}'.format(GetHumanReadable(psutil.Process(os.getpid()).memory_info().rss)))\n",
    "\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        print(' - Memory usage on epoch end: {}'.format(GetHumanReadable(psutil.Process(os.getpid()).memory_info().rss)))\n",
    "    \n",
    "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# model = tuner.hypermodel.build(best_hps)\n",
    "final_opt = tfa.optimizers.Lookahead(tfa.optimizers.LazyAdam(),\n",
    "                                     sync_period=5, slow_step_size=.5)\n",
    "# new_model.compile(optimizer=final_opt, loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#                   metrics=tf.keras.metrics.CategoricalAccuracy())\n",
    "# new_model.compile(optimizer=final_opt, loss=tf.keras.losses.MeanSquaredError())\n",
    "history = model_2.fit(dt_train, validation_data=dt_val, epochs=200,\n",
    "                    callbacks=[MemoryUsageCallback(),\n",
    "                               reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35d6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "64628e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_model = tf.keras.models.load_model(filepath=os.path.join(os.getcwd(), 'Model Checkpoint', 'v2 (Back Azimuth)',\n",
    "#              'Model Checkpoint Best', 'Model_Epoch_5'))\n",
    "# best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bce2001f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(history.history['f1_score'][:5])\n",
    "# plt.plot(history.history['val_f1_score'][:5])\n",
    "# plt.title('model f1_score')\n",
    "# plt.ylabel('f1_score')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'][5:])\n",
    "plt.plot(history.history['val_loss'][5:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2d2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ac3ebfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(model.predict(dt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6a631fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00281795,  0.0635457 ],\n",
       "       [-0.01517533,  0.00731104],\n",
       "       [ 0.00129437,  0.04663407],\n",
       "       ...,\n",
       "       [-0.00781078, -0.00079069],\n",
       "       [-0.0427766 ,  0.04510085],\n",
       "       [ 0.00332094,  0.04543925]], dtype=float32)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "467e9e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16831734, -0.98573286],\n",
       "       [ 0.97029573,  0.2419219 ],\n",
       "       [ 0.14228343,  0.98982596],\n",
       "       ...,\n",
       "       [ 0.57785762,  0.81613759],\n",
       "       [-0.56063899,  0.82806033],\n",
       "       [ 0.89802758, -0.43993917]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e13c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "36e7f8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted-RSquared Azimuth  : -60.9 %\n",
      "\n",
      "95.3% Confident Interval Magnitude: -3.88 <> 4.7 (Degree)\n"
     ]
    }
   ],
   "source": [
    "def decoder_atan2(data):\n",
    "    sin_ = data[:, 0]\n",
    "    cos_ = data[:, 1]\n",
    "    radian_backazimuth = []\n",
    "    for i in range(data.shape[0]):\n",
    "        radian_backazimuth.append(math.atan2(sin_[i], cos_[i]))\n",
    "    return np.array(radian_backazimuth)\n",
    "\n",
    "X_true = np.array(np.concatenate([x for x, y in dt_test], axis=0))\n",
    "y_true = np.array(np.concatenate([y for x, y in dt_test], axis=0))\n",
    "y_pred = np.array(model.predict(dt_test))\n",
    "y_pred_az = decoder_atan2(y_pred)\n",
    "y_true_az = decoder_atan2(y_true)\n",
    "\n",
    "# metric = tfa.metrics.RSquare(num_regressors=6000)\n",
    "# metric.update_state(np.exp(y_true[:, 0])/100, np.exp(y_pred[:, 0])/100)\n",
    "# result_s = round(metric.result().numpy()*100, 2)\n",
    "\n",
    "# metric = tfa.metrics.RSquare(num_regressors=6000)\n",
    "# metric.update_state(np.exp(y_true[:, 1])-1, np.exp(y_pred[:, 1])-1)\n",
    "# result_mag = round(metric.result().numpy()*100, 2)\n",
    "\n",
    "metric = tfa.metrics.RSquare(num_regressors=6000)\n",
    "metric.update_state(y_true_az, y_pred_az)\n",
    "result_az = round(metric.result().numpy()*100, 2)\n",
    "\n",
    "# print(f'\\nAdjusted-RSquared S-Wave : {result_s} %')\n",
    "# print(f'Adjusted-RSquared Magnitude: {result_mag} %')\n",
    "print(f'Adjusted-RSquared Azimuth  : {result_az} %\\n')\n",
    "\n",
    "# true_s = np.exp(y_true[:, 0])/100\n",
    "# true_mag = np.exp(y_true[:, 1])-1\n",
    "# pred_s = np.exp(y_pred[:, 0])/100\n",
    "# pred_mag = np.exp(y_pred[:, 1])-1\n",
    "# true_az = y_true[:, 2]\n",
    "# pred_az = y_pred[:, 2]\n",
    "\n",
    "# error_s = true_s-pred_s\n",
    "# error_mag = true_mag-pred_mag\n",
    "error_az = y_true_az-y_pred_az\n",
    "\n",
    "# mean_error_s = np.mean(error_s)\n",
    "# std_error_s = np.std(error_s)\n",
    "\n",
    "# mean_error_mag = np.mean(error_mag)\n",
    "# std_error_mag = np.std(error_mag)\n",
    "\n",
    "mean_error_az = np.mean(error_az)\n",
    "std_error_az = np.std(error_az)\n",
    "\n",
    "# print(f'95.3% Confident Interval S-Wave: {round(mean_error_s-2*std_error_s, 2)} <> {round(mean_error_s+2*std_error_s, 2)} (Second)')\n",
    "# print(f'95.3% Confident Interval Magnitude: {round(mean_error_mag-2*std_error_mag, 2)} <> {round(mean_error_mag+2*std_error_mag, 2)} (Schala Richter)')\n",
    "print(f'95.3% Confident Interval Magnitude: {round(mean_error_az-2*std_error_az, 2)} <> {round(mean_error_az+2*std_error_az, 2)} (Degree)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edbe2500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1442.3329280806315"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b9db9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-248221.16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6e344788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5177284 , -0.85554503],\n",
       "       [-0.7313537 ,  0.68199836],\n",
       "       [-0.78574895,  0.61854554],\n",
       "       [-0.45709793,  0.88941637],\n",
       "       [ 0.99472074, -0.10261893]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8f123dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.597398990420469"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(math.atan2(0.5177284, -0.85554503))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5a5dfc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445420    2.597399\n",
       "530363   -0.820305\n",
       "916079   -0.903906\n",
       "944529   -0.474730\n",
       "875801    1.673596\n",
       "            ...   \n",
       "60096     2.926917\n",
       "824728   -1.966812\n",
       "229288   -2.806489\n",
       "190924    2.600541\n",
       "536022    1.937315\n",
       "Name: back_azimuth_deg, Length: 257, dtype: float64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_backAzimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "eaabc360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8718127964790214"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(5.771280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "76dcbaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-1.9111356>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_atan2(-0.33380686, -0.94264149)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5e0d2635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s_arrival_sample    6.688355\n",
       "source_magnitude    1.458615\n",
       "back_azimuth_deg    4.372050\n",
       "sin                -0.942641\n",
       "cos                -0.333807\n",
       "Name: 311340, dtype: float64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.loc[311340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "31724ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250.50001282017155"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.degrees(4.372050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "85209052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.5"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "250.50-180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bad3064c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2304571226560024"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.radians(70.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9ced76fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9426414910921784"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sin(1.2304571226560024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "83c029b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3338068592337709"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(1.2304571226560024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "56451020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.2304572>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_atan2(0.3338068592337709, 0.9426414910921784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e2e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
